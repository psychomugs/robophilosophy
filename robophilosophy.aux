\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{breazeal2004designing}
\citation{dautenhahn1999bringing}
\citation{duffy2003anthropomorphism}
\citation{fong2003survey}
\citation{giger2019humanization}
\citation{mays2021humanizing}
\citation{ankivector}
\citation{jibo}
\citation{kuri}
\citation{komatsu2012expectations}
\citation{sandoval2014human}
\citation{nishio2007geminoid}
\citation{hansonsophia}
\citation{mori1970uncanny}
\global\@namedef{n@author@}{1}
\citation{mori1970uncanny}
\citation{affinity}
\citation{macdorman2005mortality}
\citation{mori2012uncanny}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:related.bukimi}{{1}{}{Our proposed approach for humanizing the robot, referencing Mori's \textit {bukimi no tani}~(不気味の谷, ``the valley of eerieness,'' anglicized as ``the uncanny valley'') as a conceptual framework \cite {mori1970uncanny}. I equate ``humanizing'' with maximizing ``human affinity'' on the vertical axis of the graph (left green arrow), and journey out of the valley by making accessible (gray arrow) three phases of robot development: design, movement, and telepresence.\relax }{figure.caption.1}{}}
\citation{jacob2012yeah}
\citation{darling2021new}
\citation{cascone2000aesthetics}
\citation{cramer2015post}
\citation{simbelis2018humanizing}
\citation{thibault2018made}
\citation{shannon1948communication}
\citation{mcluhan1964understanding}
\citation{hoorn2020medium}
\citation{henkel2014photo}
\citation{turkle2017alone}
\citation{calo2011ethical}
\citation{williams2021misfit}
\citation{shannon1948communication}
\newlabel{fig:blossom_comm}{{2}{}{Blossom's journey out of the uncanny valley (left) and interpreting each phase (design, movement, telepresence) as forms of robot-mediated communication, as formalized through Shannon's model of communication (right) \cite {shannon1948communication}.\relax }{figure.caption.2}{}}
\citation{gaver2012annotated}
\citation{suguitan2019blossom}
\citation{thomas1981illusion}
\citation{friedman2021clothing}
\newlabel{fig:design.anno}{{3}{}{Annotated portfolio \cite {gaver2012annotated} of Blossom and the aesthetic concepts that inspired its design.\relax }{figure.caption.3}{}}
\citation{norman2013design}
\citation{richie2007tractate}
\citation{dunnerabycritical}
\citation{walter1950imitation}
\citation{benjamin1935work}
\citation{suguitan2020moveae}
\citation{kingma2013auto}
\citation{suguitan2020moveae}
\newlabel{fig:mvmt.system}{{4}{}{The movement authoring system. Users move the phone (left), and \texttt {DeviceOrientation} transmits the motion of the phone through \texttt {ngrok} and \texttt {socket.io} to the robot. The robot's back end inverse kinematics model calculates the motor positions required to match the phone's pose. For the telepresence application, \texttt {WebRTC} transmits a first-person video feed from a wide-angle camera embedded inside the robot's head to the phone interface.\relax }{figure.caption.4}{}}
\newlabel{fig:mvmt.vae}{{5}{}{One of the behavior generation neural network models: a face→movement translation network. The movement variational autoencoder (VAE) learns compressed representations of the original movements (top left to right). An additional ResNet-based image encoder (bottom left) compresses images of facial expressions $x_f$ into the shared latent space $\{z_m,z_f\}$. Once the end-to-end network is trained, we can either generate new movements by sampling from $z_m$ and passing through the movement decoder (top left to right) or translate faces into movements by passing images through the face encoder and movement decoder (bottom left to right).\relax }{figure.caption.5}{}}
\citation{youarenottherobot2021suguitan}
\citation{suguitan2022variable}
\citation{pagliarini2009development}
\bibstyle{ieeetr}
\bibdata{master.bib}
\bibcite{breazeal2004designing}{1}
\bibcite{dautenhahn1999bringing}{2}
\bibcite{duffy2003anthropomorphism}{3}
\bibcite{fong2003survey}{4}
\bibcite{giger2019humanization}{5}
\bibcite{mays2021humanizing}{6}
\bibcite{ankivector}{7}
\bibcite{jibo}{8}
\bibcite{kuri}{9}
\bibcite{komatsu2012expectations}{10}
\bibcite{sandoval2014human}{11}
\bibcite{nishio2007geminoid}{12}
\bibcite{hansonsophia}{13}
\bibcite{mori1970uncanny}{14}
\bibcite{affinity}{15}
\bibcite{macdorman2005mortality}{16}
\bibcite{mori2012uncanny}{17}
\bibcite{jacob2012yeah}{18}
\bibcite{darling2021new}{19}
\bibcite{cascone2000aesthetics}{20}
\bibcite{cramer2015post}{21}
\bibcite{simbelis2018humanizing}{22}
\bibcite{thibault2018made}{23}
\bibcite{shannon1948communication}{24}
\bibcite{mcluhan1964understanding}{25}
\bibcite{hoorn2020medium}{26}
\bibcite{henkel2014photo}{27}
\bibcite{turkle2017alone}{28}
\bibcite{calo2011ethical}{29}
\bibcite{williams2021misfit}{30}
\bibcite{gaver2012annotated}{31}
\bibcite{suguitan2019blossom}{32}
\bibcite{thomas1981illusion}{33}
\bibcite{friedman2021clothing}{34}
\bibcite{norman2013design}{35}
\bibcite{richie2007tractate}{36}
\bibcite{dunnerabycritical}{37}
\bibcite{walter1950imitation}{38}
\bibcite{benjamin1935work}{39}
\bibcite{suguitan2020moveae}{40}
\bibcite{kingma2013auto}{41}
\bibcite{youarenottherobot2021suguitan}{42}
\bibcite{suguitan2022variable}{43}
\bibcite{pagliarini2009development}{44}
\global\@namedef{@lastpage}{10}
\gdef \@abspage@last{10}
