@article{zhu2017unpaired,
  title={Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  journal={arXiv Preprint},
  year={2017}
}

@ARTICLE{karg2013body,
author={Karg, Michelle and Samadani, Ali-Akbar and Gorbet, Rob and K{\"u}hnlenz, Kolja and Hoey, Jesse and Kuli{\'c}, Dana},
journal={IEEE Transactions on Affective Computing},
title={Body Movements for Affective Expression: A Survey of Automatic Recognition and Generation},
year={2013},
volume={4},
number={4},
pages={341-359},
keywords={emotion recognition;human computer interaction;human-robot interaction;psychology;computational models;virtual agents;automatic body movement recognition;automatic body movement generation;affective expression recognition;affective state representation;notation systems;human-computer interaction;human-robot interaction;Human computer interaction;Modulation;Encoding;Computational modeling;Physiology;Systematics;Robots;Movement analysis;recognition of affective expressions;generation of affective expressions},
doi={10.1109/T-AFFC.2013.29},
ISSN={},
month={Oct},}


@article{argall2009survey,
title = "A Survey of Robot Learning From Demonstration",
journal = "Robotics and Autonomous Systems",
volume = "57",
number = "5",
pages = "469--483",
year = "2009",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2008.10.024",
url = "http://www.sciencedirect.com/science/article/pii/S0921889008001772",
author = "Brenna D. Argall and Sonia Chernova and Manuela Veloso and Brett Browning",
keywords = "Learning from demonstration, Robotics, Machine learning, Autonomous systems",
abstract = "We present a comprehensive survey of robot Learning from Demonstration (LfD), a technique that develops policies from example state to action mappings. We introduce the LfD design choices in terms of demonstrator, problem space, policy derivation and performance, and contribute the foundations for a structure in which to categorize LfD research. Specifically, we analyze and categorize the multiple ways in which examples are gathered, ranging from teleoperation to imitation, as well as the various techniques for policy derivation, including matching functions, dynamics models and plans. To conclude we discuss LfD limitations and related promising areas for future research."
}

@article{schmidhuber2015deep,
title = "Deep Learning in Neural Networks: An Overview",
journal = "Neural Networks",
volume = "61",
pages = "85--117",
year = "2015",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2014.09.003",
url = "http://www.sciencedirect.com/science/article/pii/S0893608014002135",
author = "Schmidhuber, J{\"u}rgen",
keywords = "Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation",
abstract = "In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks."
}

@inproceedings{goodfellow2014generative,
  title={Generative Adversarial Nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2672--2680},
  year={2014}
}

@article{ma2006motion,
  title={A Motion Capture Library for the Study of Identity, Gender, and Emotion Perception From Biological Motion},
  author={Ma, Yingliang and Paterson, Helena M and Pollick, Frank E},
  journal={Behavior Research Methods},
  volume={38},
  number={1},
  pages={134--141},
  year={2006},
  publisher={Springer}
}

@inproceedings{suguitan2018blossom,
  title={Blossom: A Tensile Social Robot Design With a Handcrafted Shell},
  author={Suguitan, Michael and Hoffman, Guy},
  booktitle={Companion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={383--383},
  year={2018},
  organization={ACM}
}

@article{suguitan2019blossom,
 author = {Suguitan, Michael and Hoffman, Guy},
 title = {Blossom: A Handcrafted Open-Source Robot},
 journal = {ACM Transactions on Human-Robot Interaction},
 issue_date = {March 2019},
 volume = {8},
 number = {1},
 month = mar,
 year = {2019},
 issn = {2573-9522},
 pages = {2:1--2:27},
 articleno = {2},
 numpages = {27},
 url = {http://doi.acm.org/10.1145/3310356},
 doi = {10.1145/3310356},
 acmid = {3310356},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Robot design, craft, craft robotics, handcrafted, open-source, research platform, robot toolkit, social robotics, soft robotics, toolkit},
} 

@article{knight2016expressive,
  title={Expressive Motion for Low Degree-Of-Freedom Robots},
  author={Knight, Heather},
  year={2016}
}

@INPROCEEDINGS{zhou2018cost,
author={Zhou, Allan and Dragan, Anca D},
booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title={Cost Functions for Robot Motion Style},
year={2018},
volume={},
number={},
pages={3632-3639},
keywords={control engineering computing;learning (artificial intelligence);mobile robots;neurocontrollers;task constraints;nominal task cost;task types;task instances;robot motion style;nuanced costs;featurized costs;nominal motion;cost type;raw trajectory input;neural network parameterization operating;hand-designed features;weighted linear combination;cost functions;trajectory optimization process;Task analysis;Robots;Cost function;Neural networks;Trajectory optimization},
doi={10.1109/IROS.2018.8594433},
ISSN={},
month={Oct},}

@article{kanda2004interactive,
  title={Interactive Robots as Social Partners and Peer Tutors for Children: A Field Trial},
  author={Kanda, Takayuki and Hirano, Takayuki and Eaton, Daniel and Ishiguro, Hiroshi},
  journal={Human--Computer Interaction},
  volume={19},
  number={1-2},
  pages={61--84},
  year={2004},
  publisher={Taylor \& Francis}
}

@INPROCEEDINGS{salter2004robots,
author={Salter, Tamie and Dautenhahn, Kerstin and Bockhorst, R},
booktitle={RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)},
title={Robots Moving Out of the Laboratory - Detecting Interaction Levels and Human Contact in Noisy School Environments},
year={2004},
volume={},
number={},
pages={563-568},
keywords={mobile robots;man-machine systems;noise (working environment);educational institutions;user interfaces;infrared detectors;intelligent sensors;noisy school environments;interaction level detection;human contact detection;human-robot interaction;infrared sensors;mobile robot;Human robot interaction;Working environment noise;Noise level;Robot sensing systems;Rehabilitation robotics;Educational institutions;Mobile robots;Acoustic noise;Autism;Medical treatment},
doi={10.1109/ROMAN.2004.1374822},
ISSN={},
month={Sep.},}

@INPROCEEDINGS{gockley2005designing,
author={Gockley, Rachel and Bruce, Allison and Forlizzi, Jodi and Michalowski, Marek and Mundell, Anne and Rosenthal, Stephanie and Sellner, Brennan and Simmons, Reid and Snipes, Kevin and Schultz, Alan C and others},
booktitle={2005 IEEE/RSJ International Conference on Intelligent Robots and Systems},
title={Designing Robots for Long-Term Social Interaction},
year={2005},
volume={},
number={},
pages={1338-1343},
keywords={mobile robots;human computer interaction;robot design;long-term social interaction;roboceptionist;social robot;human-robot social interaction;long-term human-robot relationship;Human robot interaction;Facial animation;Weather forecasting;Face;Laboratories;Data analysis;Monitoring;Displays;Buildings;Legged locomotion;robotics;social robots;human-robot interaction},
doi={10.1109/IROS.2005.1545303},
ISSN={},
month={Aug},}


@inproceedings{liu2017unsupervised,
  title={Unsupervised Image-To-Image Translation Networks},
  author={Liu, Ming-Yu and Breuel, Thomas and Kautz, Jan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={700--708},
  year={2017}
}

@article{makhzani2015adversarial,
  title={Adversarial Autoencoders},
  author={Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  journal={arXiv Preprint arXiv:1511.05644},
  year={2015}
}

@article{holden2016deep,
  title={A Deep Learning Framework for Character Motion Synthesis and Editing},
  author={Holden, Daniel and Saito, Jun and Komura, Taku},
  journal={ACM Transactions on Graphics (TOG)},
  volume={35},
  number={4},
  pages={138},
  year={2016},
  publisher={ACM}
}

@inproceedings{kim2013deep,
  title={Deep Learning for Robust Feature Generation in Audiovisual Emotion Recognition},
  author={Kim, Yelin and Lee, Honglak and Provost, Emily Mower},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3687--3691},
  year={2013},
  organization={IEEE}
}

@article{rhodin2014interactive,
author = {Rhodin, Helge and Tompkin, James and In Kim, Kwang and Varanasi, Kiran and Seidel, Hans-Peter and Theobalt, Christian},
title = {Interactive Motion Mapping for Real-Time Character Control},
journal = {Computer Graphics Forum},
volume = {33},
number = {2},
pages = {273-282},
doi = {10.1111/cgf.12325},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12325},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12325},
abstract = {Abstract It is now possible to capture the 3D motion of the human body on consumer hardware and to puppet in real time skeleton-based virtual characters. However, many characters do not have humanoid skeletons. Characters such as spiders and caterpillars do not have boned skeletons at all, and these characters have very different shapes and motions. In general, character control under arbitrary shape and motion transformations is unsolved - how might these motions be mapped? We control characters with a method which avoids the rigging-skinning pipeline â source and target characters do not have skeletons or rigs. We use interactively-defined sparse pose correspondences to learn a mapping between arbitrary 3D point source sequences and mesh target sequences. Then, we puppet the target character in real time. We demonstrate the versatility of our method through results on diverse virtual characters with different input motion controllers. Our method provides a fast, flexible, and intuitive interface for arbitrary motion mapping which provides new ways to control characters for real-time animation.},
year = {2014}
}

@inproceedings{seol2013creature,
 author = {Seol, Yeongho and O'Sullivan, Carol and Lee, Jehee},
 title = {Creature Features: Online Motion Puppetry for Non-Human Characters},
 booktitle = {Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
 series = {SCA '13},
 year = {2013},
 isbn = {978-1-4503-2132-7},
 location = {Anaheim, California},
 pages = {213--221},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/2485895.2485903},
 doi = {10.1145/2485895.2485903},
 acmid = {2485903},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {animation, motion, non-human character, puppetry, real-time},
} 

@inproceedings{yamane2010animating,
 author = {Yamane, Katsu and Ariki, Yuka and Hodgins, Jessica},
 title = {Animating Non-Humanoid Characters With Human Motion Data},
 booktitle = {Proceedings of the 2010 ACM SIGGRAPH/Eurographics Symposium on Computer Animation},
 series = {SCA '10},
 year = {2010},
 location = {Madrid, Spain},
 pages = {169--178},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=1921427.1921453},
 acmid = {1921453},
 publisher = {Eurographics Association},
 address = {Goslar Germany, Germany},
} 


@ARTICLE{kleinsmith2013affective,
author={Kleinsmith, Andrea and Bianchi-Berthouze, Nadia},
journal={IEEE Transactions on Affective Computing},
title={Affective Body Expression Perception and Recognition: A Survey},
year={2013},
volume={4},
number={1},
pages={15-33},
keywords={pattern recognition;affective body expression perception;affective body expression recognition;whole-body sensing technology;affective communication channel;human factor;form information;movement information;input modality;data collection;data labeling;data modeling;automatic recognition system;Face recognition;Emotion recognition;Data models;Cultural differences;Face recognition;Emotion recognition;Data models;Cultural differences;spatiotemporal affective body features;Affective body posture;affective body movement;affective recognition systems;cross-cultural differences},
doi={10.1109/T-AFFC.2012.16},
ISSN={},
month={Jan},}

@inproceedings{karg2010towards,
  title={Towards Mapping Emotive Gait Patterns From Human to Robot},
  author={Karg, Michelle and Schwimmbeck, Mathias and K{\"u}hnlenz, Kolja and Buss, Martin},
  booktitle={Robot and Human Interactive Communication (RO-MAN)},
  pages={258--263},
  year={2010},
  organization={IEEE}
}

@article{ogata2010inter,
  title={Inter-Modality Mapping in Robot With Recurrent Neural Network},
  author={Ogata, Tetsuya and Nishide, Shun and Kozima, Hideki and Komatani, Kazunori and Okuno, Hiroshi G},
  journal={Pattern Recognition Letters},
  volume={31},
  number={12},
  pages={1560--1569},
  year={2010},
  publisher={Elsevier}
}


@article{bahdanau2014neural,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv Preprint arXiv:1409.0473},
  year={2014}
}

@article{cho2014learning,
  title={Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv Preprint arXiv:1406.1078},
  year={2014}
}

@inproceedings{xu2015show,
  title={Show, Attend and Tell: Neural Image Caption Generation With Visual Attention},
  author={Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={2048--2057},
  year={2015}
}

@inproceedings{wan2017crossing,
  title={Crossing Nets: Combining Gans and Vaes With a Shared Latent Space for Hand Pose Estimation},
  author={Wan, Chengde and Probst, Thomas and Van Gool, Luc and Yao, Angela},
  booktitle={Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  organization={IEEE}
}

@article{betella2016affective,
  title={The Affective Slider: A Digital Self-Assessment Scale for the Measurement of Human Emotions},
  author={Betella, Alberto and Verschure, Paul FMJ},
  journal={PloS One},
  volume={11},
  number={2},
  pages={e0148037},
  year={2016},
  publisher={Public Library of Science}
}

@article{bartneck2009measurement,
  title={Measurement Instruments for the Anthropomorphism, Animacy, Likeability, Perceived Intelligence, and Perceived Safety of Robots},
  author={Bartneck, Christoph and Kuli{\'c}, Dana and Croft, Elizabeth and Zoghbi, Susana},
  journal={International Journal of Social Robotics},
  volume={1},
  number={1},
  pages={71--81},
  year={2009},
  publisher={Springer}
}

@misc{kingma2013auto,
    title={Auto-Encoding Variational Bayes},
    author={Diederik P Kingma and Max Welling},
    year={2013},
    eprint={1312.6114},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein Generative Adversarial Networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International Conference on Machine Learning},
  pages={214--223},
  year={2017}
}

@article{radford2015unsupervised,
  title={Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv Preprint arXiv:1511.06434},
  year={2015}
}

@article{mirza2014conditional,
  title={Conditional Generative Adversarial Nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv Preprint arXiv:1411.1784},
  year={2014}
}

@article{durugkar2016generative,
  title={Generative Multi-Adversarial Networks},
  author={Durugkar, Ishan and Gemp, Ian and Mahadevan, Sridhar},
  journal={arXiv Preprint arXiv:1611.01673},
  year={2016}
}

@article{van2016wavenet,
  title={Wavenet: A Generative Model for Raw Audio},
  author={Van Den Oord, A{\"a}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={CoRR Abs/1609.03499},
  year={2016}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent Neural Network Based Language Model},
  author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={11th Annual Conference of the International Speech Communication Association},
  year={2010}
}

@ARTICLE{alissandrakis2007correspondence,
author={Alissandrakis, Aris and Nehaniv, Chrystopher L and Dautenhahn, Kerstin},
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
title={Correspondence Mapping Induced State and Action Metrics for Robotic Imitation},
year={2007},
volume={37},
number={2},
pages={299-307},
keywords={intelligent robots;learning (artificial intelligence);software agents;correspondence mapping;induced state;action metrics;robotic imitation;body mapping;correspondence matrices;associations;degrees of freedom;state matching;simulated 3-D robotic examples;agents;kinematic models;Mirrors;Robot programming;Humans;Computer science;Psychology;Robot kinematics;Morphology;Symmetric matrices;Education;Pediatrics;Correspondence problem;imitation and social learning;p rogramming by demonstration;state and action metrics;Algorithms;Artificial Intelligence;Biomimetics;Computer Simulation;Cybernetics;Humans;Imitative Behavior;Models, Biological;Movement;Robotics;Task Performance and Analysis},
doi={10.1109/TSMCB.2006.886947},
ISSN={},
month={April},}

@inproceedings{saerbeck2010perception,
 author = {Saerbeck, Martin and Bartneck, Christoph},
 title = {Perception of Affect Elicited by Robot Motion},
 booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Human-robot Interaction},
 series = {HRI '10},
 year = {2010},
 isbn = {978-1-4244-4893-7},
 location = {Osaka, Japan},
 pages = {53--60},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1734454.1734473},
 acmid = {1734473},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {affective communication, expressive robotic behavior, nonverbal communication},
} 


@inproceedings{mordatch2015interactive,
  title={Interactive Control of Diverse Complex Characters With Neural Networks},
  author={Mordatch, Igor and Lowrey, Kendall and Andrew, Galen and Popovic, Zoran and Todorov, Emanuel V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3132--3140},
  year={2015}
}

@article{li2017auto,
  title={Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis},
  author={Li, Zimo and Zhou, Yi and Xiao, Shuangjiu and He, Chong and Huang, Zeng and Li, Hao},
  journal={arXiv Preprint arXiv:1707.05363},
  year={2017}
}

@inproceedings{kahou2013combining,
 author = {Kahou, Samira Ebrahimi and Pal, Christopher and Bouthillier, Xavier and Froumenty, Pierre and G\"{u}l\c{c}ehre, \c{C}aglar and Memisevic, Roland and Vincent, Pascal and Courville, Aaron and Bengio, Yoshua and Ferrari, Raul Chandias and Mirza, Mehdi and Jean, S{\'e}bastien and Carrier, Pierre-Luc and Dauphin, Yann and Boulanger-Lewandowski, Nicolas and Aggarwal, Abhishek and Zumer, Jeremie and Lamblin, Pascal and Raymond, Jean-Philippe and Desjardins, Guillaume and Pascanu, Razvan and Warde-Farley, David and Torabi, Atousa and Sharma, Arjun and Bengio, Emmanuel and C\^{o}t{\'e}, Myriam and Konda, Kishore Reddy and Wu, Zhenzhou},
 title = {Combining Modality Specific Deep Neural Networks for Emotion Recognition in Video},
 booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
 series = {ICMI '13},
 year = {2013},
 isbn = {978-1-4503-2129-7},
 location = {Sydney, Australia},
 pages = {543--550},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2522848.2531745},
 doi = {10.1145/2522848.2531745},
 acmid = {2531745},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {emotion recognition},
} 

@inproceedings{ng2015deep,
 author = {Ng, Hong-Wei and Nguyen, Viet Dung and Vonikakis, Vassilios and Winkler, Stefan},
 title = {Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning},
 booktitle = {Proceedings of the 2015 ACM on International Conference on Multimodal Interaction},
 series = {ICMI '15},
 year = {2015},
 isbn = {978-1-4503-3912-4},
 location = {Seattle, Washington, USA},
 pages = {443--449},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/2818346.2830593},
 doi = {10.1145/2818346.2830593},
 acmid = {2830593},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deep learning networks, emotion classification, facial expression analysis},
} 


@article{ghayoumi2016architecture,
  title={Architecture of Emotion in Robots Using Convolutional Neural Networks},
  author={Ghayoumi, Mehdi and Bansal, Arvind K},
  journal={Rss, Usa},
  year={2016}
}

@incollection{susskind2008generating,
  title={Generating Facial Expressions With Deep Belief Nets},
  author={Susskind, Joshua M and Hinton, Geoffrey E and Movellan, Javier R and Anderson, Adam K},
  booktitle={Affective Computing},
  year={2008},
  publisher={InTech}
}

@INPROCEEDINGS{lee2017chatbot,
author={ {Dongkeon Lee} and {Kyo-Joong Oh} and {Ho-Jin Choi}},
booktitle={2017 IEEE International Conference on Big Data and Smart Computing (BigComp)},
title={The Chatbot Feels You - A Counseling Service Using Emotional Response Generation},
year={2017},
volume={},
number={},
pages={437-440},
keywords={emotion recognition;ethical aspects;health care;natural language processing;psychology;drinking habit;intervene chatbot;continuous emotion recognition;mental healthcare experiment;clinical psychiatric consolation;psychiatric counseling service;natural language processing;conversation content;NLP;emotional flow;continuous conversation observation;personalized counseling response;user input;user emotion;expected reaction;ethical view;emotional response generation;Emotion recognition;Employee welfare;Medical services;Context;Natural language processing;Data models;Probabilistic logic;conversational service;response generation;deep learning},
doi={10.1109/BIGCOMP.2017.7881752},
ISSN={},
month={Feb},}

@article{kamnitsas2018semi,
  title={Semi-Supervised Learning via Compact Latent Space Clustering},
  author={Kamnitsas, Konstantinos and Castro, Daniel C and Folgoc, Loic Le and Walker, Ian and Tanno, Ryutaro and Rueckert, Daniel and Glocker, Ben and Criminisi, Antonio and Nori, Aditya},
  journal={arXiv Preprint arXiv:1806.02679},
  year={2018}
}

@inproceedings{mandery2015kit,
  title={The KIT Whole-Body Human Motion Database},
  author={Mandery, Christian and Terlemez, {\"O}mer and Do, Martin and Vahrenkamp, Nikolaus and Asfour, Tamim},
  booktitle={Advanced Robotics (ICAR)},
  pages={329--336},
  year={2015},
  organization={IEEE}
}

@article{volkova2014mpi,
  title={The MPI Emotional Body Expressions Database for Narrative Scenarios},
  author={Volkova, Ekaterina and De La Rosa, Stephan and B{\"u}lthoff, Heinrich H and Mohler, Betty},
  journal={PloS One},
  volume={9},
  number={12},
  pages={e113647},
  year={2014},
  publisher={Public Library of Science}
}

@article{de2008guide,
  title={Guide to the Carnegie Mellon University Multimodal Activity (Cmu-Mmac) Database},
  author={De la Torre, Fernando and Hodgins, Jessica and Bargteil, Adam and Martin, Xavier and Macey, Justin and Collado, Alex and Beltran, Pep},
  journal={Robotics Institute},
  pages={135},
  year={2008}
}

@Article{poel2009gaze,
author={Poel, Mannes and Heylen, Dirk and Nijholt, Anton and Meulemans, M and Van Breemen, A},
title="Gaze Behaviour, Believability, Likability and the iCat",
journal="Ai {\&} Society",
year="2009",
month="Aug",
day="01",
volume="24",
number="1",
pages="61--73",
abstract="The iCat is a user-interface robot with the ability to express a range of emotions through its facial features. This article summarizes our research to see whether we can increase the believability and likability of the iCat for its human partners through the application of gaze behaviour. Gaze behaviour serves several functions during social interaction such as mediating conversation flow, communicating emotional information and avoiding distraction by restricting visual input. There are several types of eye and head movements that are necessary for realizing these functions. We designed and evaluated a gaze behaviour system for the iCat robot that implements realistic models of the major types of eye and head movements found in living beings: vergence, vestibulo ocular reflexive, smooth pursuit movements and gaze shifts. We discuss how these models are integrated into the software environment of the iCat and can be used to create complex interaction scenarios. We report about some user tests and draw conclusions for future evaluation scenarios.",
issn="1435-5655",
doi="10.1007/s00146-009-0198-1",
url="https://doi.org/10.1007/s00146-009-0198-1"
}



@INPROCEEDINGS{hashimoto2008effect,
author={Hashimoto, Minoru and Kondo, Hiromi and Tamatsu, Yukimasa},
booktitle={RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication},
title={Effect of Emotional Expression to Gaze Guidance Using a Face Robot},
year={2008},
volume={},
number={},
pages={95-100},
keywords={emotion recognition;mobile robots;emotional expression;gaze guidance;face robot;emotional motion;head robot;Kamin-FA1;gaze control;facial expression function;curved surface display;gaze measurement;Robots},
doi={10.1109/ROMAN.2008.4600649},
ISSN={},
month={Aug},}

@article{nehaniv2001like,
  title={Like Me?-Measures of Correspondence and Imitation},
  author={Nehaniv, Chrystopher L and Dautenhahn, Kerstin},
  journal={Cybernetics \& Systems},
  volume={32},
  number={1-2},
  pages={11--51},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{alissandrakis2005imitating,
  title={Imitating Using JABBERWOCKY to Achieve Corresponding Effects in Context},
  author={Alissandrakis, Aris and Nehaniv, Chrystopher L and Dautenhahn, Kerstin and Saunders, Joe},
  journal={Univ. Hertfordshire, Hatfield, UK, Computer Science Tech. Rep},
  volume={441},
  year={2005}
}

@misc{larsen2015autoencoding,
    title={Autoencoding Beyond Pixels Using a Learned Similarity Metric},
    author={Anders Boesen Lindbo Larsen and S{\o}ren Kaae S{\o}nderby and Hugo Larochelle and Ole Winther},
    year={2015},
    eprint={1512.09300},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{roberts2018musicvae,
  author    = {Adam Roberts and
               Jesse H. Engel and
               Colin Raffel and
               Curtis Hawthorne and
               Douglas Eck},
  title     = {A Hierarchical Latent Vector Model for Learning Long-Term Structure
               in Music},
  journal   = {CoRR},
  volume    = {abs/1803.05428},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.05428},
  archivePrefix = {arXiv},
  eprint    = {1803.05428},
  timestamp = {Mon, 22 Jul 2019 13:51:23 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-05428},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{higgins2017beta,
  title={Beta-Vae: Learning Basic Visual Concepts With a Constrained Variational Framework.},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  journal={Iclr},
  volume={2},
  number={5},
  pages={6},
  year={2017}
}

@inproceedings{desai2019geppetto,
 author = {Desai, Ruta and Anderson, Fraser and Matejka, Justin and Coros, Stelian and McCann, James and Fitzmaurice, George and Grossman, Tovi},
 title = {Geppetto: Enabling Semantic Design of Expressive Robot Behaviors},
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
 series = {CHI '19},
 year = {2019},
 isbn = {978-1-4503-5970-2},
 location = {Glasgow, Scotland Uk},
 pages = {369:1--369:14},
 articleno = {369},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3290605.3300599},
 doi = {10.1145/3290605.3300599},
 acmid = {3300599},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {expressive robots, robots, semantic design, semantic editing},
} 

@misc{chollet2015,
author = {Fran{\c c}ois Chollet },
title = {Keras},
year = {2015},
publisher = {GitHub},
journal = {GitHub Repository},
howpublished = {\url{https://github.com/fchollet/keras}},
commit = {04cbccc}
}

@misc{kingma2014adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{zhang2017mixup,
    title={Mixup: Beyond Empirical Risk Minimization},
    author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
    year={2017},
    eprint={1710.09412},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{desai2019geppetto,
  title={Geppetto: Enabling Semantic Design of Expressive Robot Behaviors},
  author={Desai, Ruta and Anderson, Fraser and Matejka, Justin and Coros, Stelian and McCann, James and Fitzmaurice, George and Grossman, Tovi},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={369},
  year={2019},
  organization={ACM}
}

@article{rodriguez2019spontaneous,
title = "Spontaneous Talking Gestures Using Generative Adversarial Networks",
journal = "Robotics and Autonomous Systems",
volume = "114",
pages = "57--65",
year = "2019",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2018.11.024",
url = "http://www.sciencedirect.com/science/article/pii/S0921889018304445",
author = "Rodriguez, Igor and Martínez-Otzeta, Jos{\'e} María and Irigoien, Itziar and Lazkano, Elena",
keywords = "Social robotics, Generative learning models, Motion generation, Principal coordinate analysis, Body language expression, Generative adversarial networks",
abstract = "This paper presents a talking gesture generation system based on Generative Adversarial Networks, along with an evaluation of its adequateness. The talking gesture generation system produces a sequence of joint positions of the robotâs upper body which keeps in step with an uttered sentence. The suitability of the approach is demonstrated with a real robot. Besides, the motion generation method is compared with other (non-deep) generative approaches. A two-step comparison is made. On the one hand, a statistical analysis is performed over movements generated by each approach by means of Principal Coordinate Analysis. On the other hand, the robot motion adequateness is measured by calculating the end effectorsâ jerk, path lengths and 3D space coverage."
}

@article{knight2016expressive,
  title={Expressive Motion for Low Degree-Of-Freedom Robots},
  author={Knight, Heather},
  year={2016}
}

@article{rosenblatt1958perceptron,
  title={The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.},
  author={Rosenblatt, Frank},
  journal={Psychological Review},
  volume={65},
  number={6},
  pages={386--408},
  year={1958},
  publisher={American Psychological Association}
}

@inproceedings{szegedy2017inception,
  title={Inception-V4, Inception-Resnet and the Impact of Residual Connections on Learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{mikolov2010recurrent,
  title={Recurrent Neural Network Based Language Model},
  author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Eleventh annual conference of the international speech communication association},
  year={2010}
}

@inproceedings{hinton1994autoencoders,
 author = {Hinton, Geoffrey E. and Zemel, Richard S.},
 title = {Autoencoders, Minimum Description Length and Helmholtz Free Energy},
 booktitle = {Proceedings of the 6th International Conference on Neural Information Processing Systems},
 series = {NIPS'93},
 year = {1993},
 location = {Denver, Colorado},
 pages = {3--10},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2987189.2987190},
 acmid = {2987190},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@incollection{krizhevsky2012imagenet,
title = {ImageNet Classification With Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}


@article{ekman1992argument,
author = { Paul   Ekman },
title = {An Argument for Basic Emotions},
journal = {Cognition and Emotion},
volume = {6},
number = {3-4},
pages = {169-200},
year  = {1992},
publisher = {Routledge},
doi = {10.1080/02699939208411068},

URL = { 
        https://doi.org/10.1080/02699939208411068
    
}
}

@article{maaten2008visualizing,
  title={Visualizing Data Using T-Sne},
  author={Maaten, Laurens van der and Hinton, Geoffrey},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={Nov},
  pages={2579--2605},
  year={2008}
}

@Manual{bokeh,
title = {Bokeh: Python Library for Interactive Visualization},
author = {{Bokeh Development Team}},
year = {2018},
url = {https://bokeh.pydata.org/en/latest/},
}

@article{posner2005circumplex,
  title={The circumplex model of affect: An integrative approach to affective neuroscience,
  cognitive development, and psychopathology},
  volume={17},
  DOI={10.1017/S0954579405050340},
  number={3},
  journal={Development and Psychopathology},
  publisher={Cambridge University Press},
  author={Posner, Jonathan and Russell, James A. and Peterson, Bradley S.},
  year={2005},
  pages={715--734}
}

@misc{smilkov2016embedding,
    title={Embedding Projector: Interactive Visualization and Interpretation of Embeddings},
    author={Daniel Smilkov and Nikhil Thorat and Charles Nicholson and Emily Reif and Fernanda B. Viégas and Martin Wattenberg},
    year={2016},
    eprint={1611.05469},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{zhao2017towards,
  title={Towards Deeper Understanding of Variational Autoencoding Models},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  journal={arXiv Preprint arXiv:1702.08658},
  year={2017}
}

@inproceedings{admoni2011robot,
  title={Robot Gaze Does Not Reflexively Cue Human Attention},
  author={Admoni, Henny and Bank, Caroline and Tan, Joshua and Toneva, Mariya and Scassellati, Brian},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={33},
  number={33},
  year={2011}
}

@article{horn1987closed,
  title={Closed-Form Solution of Absolute Orientation Using Unit Quaternions},
  author={Horn, Berthold KP},
  journal={Josa A},
  volume={4},
  number={4},
  pages={629--642},
  year={1987},
  publisher={Optical Society of America}
}

@article{kozima2009keepon,
  title={Keepon},
  author={Kozima, Hideki and Michalowski, Marek P and Nakagawa, Cocoro},
  journal={International Journal of Social Robotics},
  volume={1},
  number={1},
  pages={3--18},
  year={2009},
  publisher={Springer}
}

@inproceedings{salomons2018humans,
  title={Humans Conform to Robots: Disambiguating Trust, Truth, and Conformity},
  author={Salomons, Nicole and van der Linden, Michael and Strohkorb Sebo, Sarah and Scassellati, Brian},
  booktitle={Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={187--195},
  year={2018},
  organization={ACM}
}

@inproceedings{ribeiro2012illusion,
  title={The Illusion of Robotic Life: Principles and Practices of Animation for Robots},
  author={Ribeiro, Tiago and Paiva, Ana},
  booktitle={Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction},
  pages={383--390},
  year={2012},
  organization={ACM}
}

@inproceedings{bartneck2007daisy,
  title={Daisy, Daisy, Give Me Your Answer Do!: Switching Off a Robot},
  author={Bartneck, Christoph and Van Der Hoek, Michel and Mubin, Omar and Al Mahmud, Abdullah},
  booktitle={Proceedings of the ACM/IEEE international conference on Human-robot interaction},
  pages={217--222},
  year={2007},
  organization={ACM}
}

@inproceedings{castellano2009detecting,
  title={Detecting User Engagement With a Robot Companion Using Task and Social Interaction-Based Features},
  author={Castellano, Ginevra and Pereira, Andr{\'e} and Leite, Iolanda and Paiva, Ana and McOwan, Peter W},
  booktitle={Proceedings of the 2009 international conference on Multimodal interfaces},
  pages={119--126},
  year={2009},
  organization={ACM}
}

@inproceedings{kulk2008low,
  title={A Low Power Walk for the NAO Robot},
  author={Kulk, Jason and Welsh, James and others},
  booktitle={Proceedings of the 2008 Australasian Conference on Robotics \& Automation (ACRA-2008), J. Kim and R. Mahony, Eds},
  pages={1--7},
  year={2008}
}

@inproceedings{shamsuddin2012initial,
  title={Initial Response of Autistic Children in Human-Robot Interaction Therapy With Humanoid Robot NAO},
  author={Shamsuddin, Syamimi and Yussof, Hanafiah and Ismail, Luthffi and Hanapiah, Fazah Akhtar and Mohamed, Salina and Piah, Hanizah Ali and Zahari, Nur Ismarrubie},
  booktitle={Signal Processing and its Applications (CSPA), 2012 IEEE 8th International Colloquium on},
  pages={188--193},
  year={2012},
  organization={IEEE}
}

@online{suguitan2018blossompublic,
author={Michael Suguitan and Guy Hoffman},
title = {Blossom Public Repository},
url = {https://github.com/hrc2/blossom-public},
year={2018}
}

@inproceedings{zhang2016cardboardizer,
  title={CardBoardiZer: Creatively Customize, Articulate and Fold 3D Mesh Models},
  author={Zhang, Yunbo and Gao, Wei and Paredes, Luis and Ramani, Karthik},
  booktitle={Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
  pages={897--907},
  year={2016},
  organization={ACM}
}

@inproceedings{lasseter1987principles,
  title={Principles of Traditional Animation Applied to 3D Computer Animation},
  author={Lasseter, John},
  booktitle={ACM Siggraph Computer Graphics},
  volume={21},
  number={4},
  pages={35--44},
  year={1987},
  organization={ACM}
}

@inproceedings{takayama2011expressing,
  title={Expressing Thought: Improving Robot Readability With Animation Principles},
  author={Takayama, Leila and Dooley, Doug and Ju, Wendy},
  booktitle={Human-Robot Interaction (HRI), 2011 6th ACM/IEEE International Conference on},
  pages={69--76},
  year={2011},
  organization={IEEE}
}

@inproceedings{zeglin2014herb,
  title={HERB's Sure Thing: A Rapid Drama System for Rehearsing and Performing Live Robot Theater},
  author={Zeglin, Garth and Walsman, Aaron and Herlant, Laura and Zheng, Zhaodong and Guo, Yuyang and Koval, Michael C and Lenzo, Kevin and Tay, Hui Jun and Velagapudi, Prasanna and Correll, Katie and others},
  booktitle={Advanced robotics and its social impacts (ARSO), 2014 IEEE Workshop on},
  pages={129--136},
  year={2014},
  organization={IEEE}
}

@inproceedings{oh2015foldmecha,
  title={FoldMecha: Design for Linkage-Based Paper Toys},
  author={Oh, Hyunjoo and Gross, Mark D and Eisenberg, Michael},
  booktitle={Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software \& Technology},
  pages={91--92},
  year={2015},
  organization={ACM}
}

@inproceedings{lapeyre2014poppy,
  title={Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Education and Art},
  author={Lapeyre, Matthieu and Rouanet, Pierre and Grizou, Jonathan and Nguyen, Steve and Depraetre, Fabien and Le Falher, Alexandre and Oudeyer, Pierre-Yves},
  booktitle={Digital Intelligence 2014},
  pages={6},
  year={2014}
}

@online{Nintendo2018,
title = {Nintendo {Labo} for the {Nintendo Switch} Home Gaming System},
author = {Nintendo},
url = {https://labo.nintendo.com/},
year={2018}
}

@online{MistyRob79,
title = {Misty Robotics},
url = {https://www.mistyrobotics.com/},
year={2018}
}

@online{CES2017W60,
title = {CES 2017: Why Every Social Robot at CES Looks Alike - IEEE Spectrum},
author={Evan Ackerman},
url = {https://spectrum.ieee.org/tech-talk/robotics/home-robots/ces-2017-why-every-social-robot-at-ces-looks-alike},
year={2017}
}
@online{analogfilm,
title = {This Is Why Film Photography Is Making a Comeback},
author = {Olivier Laurent},
url = {https://time.com/4649188/film-photography-industry-comeback/},
year={2017}
}
@online{jibomourning,
title = {They Welcomed a Robot Into Their Family, Now They're Mourning Its Death},
author={Ashley Carman},
url = {https://www.theverge.com/2019/6/19/18682780/jibo-death-server-update-social-robot-mourning},
year={2019}
}
@online{kuricancel,
title = {Mayfield Robotics Cancels Kuri Social Home Robot},
author={Evan Ackerman},
url = {https://spectrum.ieee.org/mayfield-robotics-cancels-kuri-social-home-robot},
year={2018}
}
@online{ankishutdown,
title = {Consumer Robotics Company Anki Abruptly Shuts Down},
author={Evan Ackerman},
url = {https://spectrum.ieee.org/consumer-robotics-company-anki-abruptly-shuts-down},
year={2019}
}
@online{jibodead,
title = {Jibo Is Probably Totally Dead Now},
author={Evan Ackerman},
url = {https://spectrum.ieee.org/jibo-is-probably-totally-dead-now},
year={2019}
}
@online{goertzel2017sophia,
title = {Sophia the robot’s co-creator says the bot may not be true AI, but it is a work of art},
author={James Vincent},
url = {https://www.theverge.com/2017/11/10/16617092/sophia-the-robot-citizen-ai-hanson-robotics-ben-goertzel},
year={2017}
}
@online{aibofuneral,
title = {In Japan, a Buddhist Funeral Service for Robot Dogs},
author={James Burch},
url = {https://www.nationalgeographic.com/travel/destinations/asia/japan/in-japan--a-buddhist-funeral-service-for-robot-dogs/#close},
year={2018}
}

@online{LGCES20170,
title = {LG Exploring New Commercial Opportunities With Expanding Robot Portfolio},
author={LG},
url = {http://www.lg.com/au/ces2018/lg-exploring-new-commercial-opportunities-with-expanding-robot-portfolio.jsp},
year={2018}
}

@online{HondaGlo62,
title = {Honda 3E},
author = {Honda},
url = {https://global.honda/innovation/CES/2018.html},
year={2018}
}

@online{ASUSZenb50,
title = {Zenbo - Your Smart Little Companion},
author={ASUS},
url = {https://zenbo.asus.com/},
year={2018}
}

@article{park2014warm,
  title={I Am a Warm Robot: The Effects of Temperature in Physical Human--Robot Interaction},
  author={Park, Eunil and Lee, Jaeryoung},
  journal={Robotica},
  volume={32},
  number={1},
  pages={133--142},
  year={2014},
  publisher={Cambridge University Press}
}

@inproceedings{baraka2016expressive,
  title={Expressive Lights for Revealing Mobile Service Robot State},
  author={Baraka, Kim and Paiva, Ana and Veloso, Manuela},
  booktitle={Robot 2015: Second Iberian Robotics Conference},
  pages={107--119},
  year={2016},
  organization={Springer}
}

@article{Gouaillier2009,
abstract = {This article presents the mechatronic design of the autonomous humanoid robot called NAO that is built by the French company Aldebaran-Robotics. With its height of 0.57 m and its weight about 4.5 kg, this innovative robot is lightweight and compact. It distinguishes itself from existing humanoids thanks to its pelvis kinematics design, its proprietary actuation system based on brush DC motors, its electronic, computer and distributed software architectures. This robot has been designed to be affordable without sacrificing quality and performance. It is an open and easy-to-handle platform. The comprehensive and functional design is one of the reasons that helped select NAO to replace the AIBO quadrupeds in the 2008 RoboCup standard league.},
author = {Gouaillier, David and Hugel, Vincent and Blazevic, Pierre and Kilner, Chris and Monceaux, Jerome and Lafourcade, Pascal and Marnier, Brice and Serre, Julien and Maisonnier, Bruno},
doi = {10.1109/ROBOT.2009.5152516},
file = {:Users/PsychoMugs/Library/Application Support/Mendeley Desktop/Downloaded/11ed5db5c90d8e1c9134bf59245d73e8909d6c62.pdf:pdf},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
journal = {2009 IEEE International Conference on Robotics and Automation},
number = {September 2015},
pages = {769--774},
title = {{Mechatronic Design of NAO Humanoid}},
url = {http://ieeexplore.ieee.org/document/5152516/},
year = {2009}
}


@article{norton2012ikea,
  title={The IKEA Effect: When Labor Leads to Love},
  author={Norton, Michael I and Mochon, Daniel and Ariely, Dan},
  journal={Journal of Consumer Psychology},
  volume={22},
  number={3},
  pages={453--460},
  year={2012},
  publisher={Elsevier}
}
@inproceedings{breazeal1999build,
  title={How to Build Robots That Make Friends and Influence People},
  author={Breazeal, Cynthia and Scassellati, Brian},
  booktitle={Intelligent Robots and Systems, 1999. IROS'99. Proceedings. 1999 IEEE/RSJ International Conference on},
  volume={2},
  pages={858--863},
  year={1999},
  organization={IEEE}
}

@book{breazeal2004designing,
  title={Designing Sociable Robots},
  author={Breazeal, Cynthia L},
  year={2004},
  publisher={MIT Press}
}

@inproceedings{bruce2002role,
  title={The Role of Expressiveness and Attention in Human-Robot Interaction},
  author={Bruce, Allison and Nourbakhsh, Illah and Simmons, Reid},
  booktitle={Robotics and Automation, 2002. Proceedings. ICRA'02. IEEE International Conference on},
  volume={4},
  pages={4138--4142},
  year={2002},
  organization={IEEE}
}

@inproceedings{disalvo2002all-robots,
  Address = {New York, New York, USA},
  Author = {DiSalvo, Carl F. and Gemperle, Francine and Forlizzi, Jodi and Kiesler, Sara},
  Booktitle = {Proc of the 4th conference on designing interactive systems (DIS2002)},
  Doi = {10.1145/778712.778756},
  Isbn = {1581135157},
  Pages = {321--326},
  Publisher = {ACM Press},
  Title = {All robots are not created equal: the design and perception of humanoid robot heads},
  Year = {2002},
}

@article{bennett2014deriving,
  title={Deriving Minimal Features for Human-Like Facial Expressions in Robotic Faces},
  author={Bennett, Casey C and {\v{S}}abanovi{\'c}, Selma},
  journal={International Journal of Social Robotics},
  volume={6},
  number={3},
  pages={367--381},
  year={2014},
  publisher={Springer}
}

@inproceedings{kalegina2018characterizing,
  title={Characterizing the Design Space of Rendered Robot Faces},
  author={Kalegina, Alisa and Schroeder, Grace and Allchin, Aidan and Berlin, Keara and Cakmak, Maya},
  booktitle={Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={96--104},
  year={2018},
  organization={ACM}
}

@article{karg2013body,
  title={Body Movements for Affective Expression: A Survey of Automatic Recognition and Generation},
  author={Karg, Michelle and Samadani, Ali-Akbar and Gorbet, Rob and K{\"u}hnlenz, Kolja and Hoey, Jesse and Kuli{\'c}, Dana},
  journal={IEEE Transactions on Affective Computing},
  volume={4},
  number={4},
  pages={341--359},
  year={2013},
  publisher={IEEE}
}

@article{rus2015design,
  title={Design, Fabrication and Control of Soft Robots},
  author={Rus, Daniela and Tolley, Michael T},
  journal={Nature},
  volume={521},
  number={7553},
  pages={467},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{argall2009survey,
  title={A Survey of Robot Learning From Demonstration},
  author={Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  journal={Robotics and Autonomous Systems},
  volume={57},
  number={5},
  pages={469--483},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{dibia2017tjbot,
  title={Tjbot: An Open Source Diy Cardboard Robot for Programming Cognitive Systems},
  author={Dibia, Victor C and Ashoori, Maryam and Cox, Aaron and Weisz, Justin D},
  booktitle={Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
  pages={381--384},
  year={2017},
  organization={ACM}
}

@inproceedings{short2014train,
  title={How to Train Your DragonBot: Socially Assistive Robots for Teaching Children About Nutrition Through Play},
  author={Short, Elaine and Swift-Spong, Katelyn and Greczek, Jillian and Ramachandran, Aditi and Litoiu, Alexandru and Grigore, Elena Corina and Feil-Seifer, David and Shuster, Samuel and Lee, Jin Joo and Huang, Shaobo and others},
  booktitle={Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on},
  pages={924--929},
  year={2014},
  organization={IEEE}
}

@article{wada2007living,
  title={Living With Seal Robots—its Sociopsychological and Physiological Influences on the Elderly at a Care House},
  author={Wada, Kazuyoshi and Shibata, Takanori},
  journal={IEEE Transactions on Robotics},
  volume={23},
  number={5},
  pages={972--980},
  year={2007},
  publisher={IEEE}
}

@inproceedings{shibata2001physical,
  title={Physical and Affective Interaction Between Human and Mental Commit Robot},
  author={Shibata, Takanori and Tanie, Kazuo},
  booktitle={Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No. 01CH37164)},
  volume={3},
  pages={2572--2577},
  year={2001},
  organization={IEEE}
}

@misc{kuri,
author = {Mayfield Robotics},
title = {Kuri},
month = {},
year = {2018},
url={https://heykuri.com}
}

@inproceedings{fraser2015ten,
  title={Ten Things We've Learned From Blockly},
  author={Fraser, Neil},
  booktitle={Blocks and Beyond Workshop (Blocks and Beyond), 2015 IEEE},
  pages={49--50},
  year={2015},
  organization={IEEE}
}

@article{resnick2009scratch,
  title={Scratch: Programming for All},
  author={Resnick, Mitchel and Maloney, John and Monroy-Hern{\'a}ndez, Andr{\'e}s and Rusk, Natalie and Eastmond, Evelyn and Brennan, Karen and Millner, Amon and Rosenbaum, Eric and Silver, Jay and Silverman, Brian and others},
  journal={Communications of the ACM},
  volume={52},
  number={11},
  pages={60--67},
  year={2009},
  publisher={ACM}
}

@misc{hexy,
author = {ArcBotics},
title = {Hexy the Hexapod},
howpublished = {\url{http://arcbotics.com/products/hexy/}},
year = {2016}
}

@article{garage2011turtlebot,
  title={Turtlebot},
  author={Willow Garage},
  journal={Website: Http://turtlebot. Com/Last Visited},
  pages={11--25},
  year={2011}
}

@online{langevin2017inmoov,
title = {InMoov Open-Source 3D Printed Life-Size Robot},
author={Gael Langevin},
url = {http://inmoov.fr/},
year={2017},
lastaccessed={August 28, 2018}
}

@article{baveye2015liris,
  title={Liris-Accede: A Video Database for Affective Content Analysis},
  author={Baveye, Yoann and Dellandrea, Emmanuel and Chamaret, Christel and Chen, Liming},
  journal={IEEE Transactions on Affective Computing},
  volume={6},
  number={1},
  pages={43--55},
  year={2015},
  publisher={IEEE}
}

@article{wang2015video,
  title={Video Affective Content Analysis: A Survey of State-Of-The-Art Methods},
  author={Wang, Shangfei and Ji, Qiang},
  journal={IEEE Transactions on Affective Computing},
  volume={6},
  number={4},
  pages={410--430},
  year={2015},
  publisher={IEEE}
}

@inproceedings{hoffman2016openwoz,
  title={Openwoz: A Runtime-Configurable Wizard-Of-Oz Framework for Human-Robot Interaction},
  author={Hoffman, Guy},
  booktitle={2016 AAAI Spring Symposium Series},
  year={2016}
}

@misc{rethinkrobotics, title={Baxter Collaborative Robots for Industrial Automation}, url={http://www.rethinkrobotics.com/baxter/}, journal={Rethink Robotics}}

@inproceedings{igarashi1999teddy,
  title={Teddy: A Sketching Interface for 3D Freeform Design},
  author={Igarashi, Takeo and Matsuoka, Satoshi and Tanaka, Hidehiko},
  booktitle={Proceedings of the 26th annual conference on Computer graphics and interactive techniques},
  pages={409--416},
  year={1999},
  organization={ACM Press/Addison-Wesley Publishing Co.}
}

@inproceedings{michalowski2007dancing,
  title={A Dancing Robot for Rhythmic Social Interaction},
  author={Michalowski, Marek P and Sabanovic, Selma and Kozima, Hideki},
  booktitle={Human-Robot Interaction (HRI), 2007 2nd ACM/IEEE International Conference on},
  pages={89--96},
  year={2007},
  organization={IEEE}
}

@online{hasselvander2017buddy,
title={Buddy: The Companion Robot Accessible to Everyone},
author={Rodolphe Hasselvander},
url={http://www.bluefrogrobotics.com/en/buddy-your-companion-robot/},
year={2017},
lastaccessed={August 28, 2018}
}

@article{vandevelde2017yourself,
  title={Do-It-Yourself Design for Social Robots: An Open-Source Hardware Platform to Encourage Innovation},
  author={Vandevelde, Cesar and Wyffels, Francis and Vanderborght, Bram and Saldien, Jelle},
  journal={IEEE Robotics \& Automation Magazine},
  volume={24},
  number={1},
  pages={86--94},
  year={2017},
  publisher={IEEE}
}


@inproceedings{raffle2004topobo,
  title={Topobo: A Constructive Assembly System With Kinetic Memory},
  author={Raffle, Hayes Solos and Parkes, Amanda J and Ishii, Hiroshi},
  booktitle={Proceedings of the SIGCHI conference on Human factors in computing systems},
  pages={647--654},
  year={2004},
  organization={ACM}
}

@inproceedings{nakagaki2016chainform,
  title={ChainFORM: A Linear Integrated Modular Hardware System for Shape Changing Interfaces},
  author={Nakagaki, Ken and Dementyev, Artem and Follmer, Sean and Paradiso, Joseph A and Ishii, Hiroshi},
  booktitle={Proceedings of the 29th Annual Symposium on User Interface Software and Technology},
  pages={87--96},
  year={2016},
  organization={ACM}
}

@incollection{nishio2007geminoid,
  title={Geminoid: Teleoperated Android of an Existing Person},
  author={Nishio, Shuichi and Ishiguro, Hiroshi and Hagita, Norihiro},
  booktitle={Humanoid robots: new developments},
  year={2007},
  publisher={InTech}
}

@inproceedings{metta2008icub,
  title={The iCub Humanoid Robot: An Open Platform for Research in Embodied Cognition},
  author={Metta, Giorgio and Sandini, Giulio and Vernon, David and Natale, Lorenzo and Nori, Francesco},
  booktitle={Proceedings of the 8th workshop on performance metrics for intelligent systems},
  pages={50--56},
  year={2008},
  organization={ACM}
}

@inproceedings{gomez2018haru,
  title={Haru: Hardware Design of an Experimental Tabletop Robot Assistant},
  author={Gomez, Randy and Szapiro, Deborah and Galindo, Kerl and Nakamura, Keisuke},
  booktitle={Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={233--240},
  year={2018},
  organization={ACM}
}

@article{vsabanovic2016socializing,
  title={Socializing Robots: Constructing Robotic Sociality in the Design and Use of the Assistive Robot {Paro}},
  author={{\v{S}}abanovi{\'c}, Selma and Chang, Wan-Ling},
  journal={AI \& Society},
  volume={31},
  number={4},
  pages={537--551},
  year={2016},
  publisher={Springer}
}

@inproceedings{marques2013myorobotics,
  title={MYOROBOTICS: A Modular Toolkit for Legged Locomotion Research Using Musculoskeletal Designs},
  author={Marques, Hugo G and Maufroy, Christophe and Lenz, Alexander and Dalamagkidis, Konstantinos and Culha, Utku},
  booktitle={6th International Symposium on Adaptive Motion of Animals and Machines (AMAM 2013)},
  year={2013},
  organization={Technische Universit{\"a}t Darmstadt}
}

@inproceedings{jung2013make,
  title={Make It Move: A Movement Design Method of Simple Standing Products Based on Systematic Mapping of Torso Movements \& Product Messages},
  author={Jung, Jinyung and Bae, Seok-Hyung and Lee, Joon Hyub and Kim, Myung-Suk},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={1279--1288},
  year={2013},
  organization={ACM}
}

@article{li2011communication,
  title={Communication of Emotion in Social Robots Through Simple Head and Arm Movements},
  author={Li, Jamy and Chignell, Mark},
  journal={International Journal of Social Robotics},
  volume={3},
  number={2},
  pages={125--142},
  year={2011},
  publisher={Springer}
}

@inproceedings{tan2016happy,
  title={Happy Moves, Sad Grooves: Using Theories of Biological Motion and Affect to Design Shape-Changing Interfaces},
  author={Tan, Haodan and Tiab, John and {\v{S}}abanovi{\'c}, Selma and Hornb{\ae}k, Kasper},
  booktitle={Proceedings of the 2016 ACM Conference on Designing Interactive Systems},
  pages={1282--1293},
  year={2016},
  organization={ACM}
}

@article{goris2011achieve,
  title={How to Achieve the Huggable Behavior of the Social Robot Probo? A Reflection on the Actuators},
  author={Goris, Kristof and Saldien, Jelle and Vanderborght, Bram and Lefeber, Dirk},
  journal={Mechatronics},
  volume={21},
  number={3},
  pages={490--500},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{breazeal2017social,
  title={Social Robots: From Research to Commercialization},
  author={Breazeal, Cynthia},
  booktitle={Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={1--1},
  year={2017},
  organization={ACM}
}

@incollection{causo2016design,
  title={Design of Robots Used as Education Companion and Tutor},
  author={Causo, Albert and Vo, Giang Truong and Chen, I-Ming and Yeo, Song Huat},
  booktitle={Robotics and Mechatronics},
  pages={75--84},
  year={2016},
  publisher={Springer}
}

@article{dautenhahn2009kaspar,
  title={KASPAR--a Minimally Expressive Humanoid Robot for Human--Robot Interaction Research},
  author={Dautenhahn, Kerstin and Nehaniv, Chrystopher L and Walters, Michael L and Robins, Ben and Kose-Bagci, Hatice and Mirza, N Assif and Blow, Mike},
  journal={Applied Bionics and Biomechanics},
  volume={6},
  number={3-4},
  pages={369--397},
  year={2009},
  publisher={Hindawi Publishing Corporation}
}

@article{fong2003survey,
  title={A Survey of Socially Interactive Robots},
  author={Fong, Terrence and Nourbakhsh, Illah and Dautenhahn, Kerstin},
  journal={Robotics and Autonomous Systems},
  volume={42},
  number={3},
  pages={143--166},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{friesen2014ductt,
  title={DuCTT: A Tensegrity Robot for Exploring Duct Systems},
  author={Friesen, Jeffrey and Pogue, Alexandra and Bewley, Thomas and de Oliveira, Mauricio and Skelton, Robert and Sunspiral, Vytas},
  booktitle={Robotics and Automation (ICRA), 2014 IEEE International Conference on},
  pages={4222--4228},
  year={2014},
  organization={IEEE}
}

@inproceedings{gaver2010prayer,
  title={The Prayer Companion: Openness and Specificity, Materiality and Spirituality},
  author={Gaver, William and Blythe, Mark and Boucher, Andy and Jarvis, Nadine and Bowers, John and Wright, Peter},
  booktitle={Proceedings of the SIGCHI conference on Human factors in computing systems},
  pages={2055--2064},
  year={2010},
  organization={ACM}
}

@inproceedings{gray2010expressive,
  title={Expressive, Interactive Robots: Tools, Techniques, and Insights Based on Collaborations},
  author={Gray, Jesse and Hoffman, Guy and Adalgeirsson, Sigurdur Orn and Berlin, Matt and Breazeal, Cynthia},
  booktitle={HRI 2010 Workshop: What do Collaborations with the Arts have to say about HRI},
  pages={21--8},
  year={2010}
}

@article{hoffman2014designing,
  title={Designing Robots With Movement in Mind},
  author={Hoffman, Guy and Ju, Wendy},
  journal={Journal of Human-Robot Interaction},
  volume={3},
  number={1},
  pages={89--122},
  year={2014}
}

@misc{jibo,
  title={Jibo},
  author={Jibo, Inc},
  year={2015},
  note={Retrieved September 18, 2017 from \url{https://www.jibo.com/}}
}
@misc{sphero,
  title={Sphero},
  author={Sphero, Inc},
  note={Retrieved September 18, 2017 from \url{https://www.sphero.com/}}
}

@inproceedings{kozima2005interactive,
  title={Interactive Robots for Communication-Care: A Case-Study in Autism Therapy},
  author={Kozima, Hideki and Nakagawa, Cocoro and Yasuda, Yuriko},
  booktitle={Robot and human interactive communication, 2005. ROMAN 2005. IEEE International Workshop on},
  pages={341--346},
  year={2005},
  organization={IEEE}
}

@inproceedings{lapeyre2014poppy,
  title={Poppy Project: Open-Source Fabrication of 3D Printed Humanoid Robot for Science, Education and Art},
  author={Lapeyre, Matthieu and Rouanet, Pierre and Grizou, Jonathan and Nguyen, Steve and Depraetre, Fabien and Le Falher, Alexandre and Oudeyer, Pierre-Yves},
  booktitle={Digital Intelligence 2014},
  pages={6},
  year={2014}
}

@misc{leka,
  author={Leka},
  title={Discover Leka!},
  year={2016},
  note={Retrieved September 18, 2017 from \url{http://leka.io/}}
}

@online{atkin2018smartibot,
title = {Smartibot: The World's First A.I. Enabled Cardboard Robot},
author={Ross Atkin},
year={2018},
url = {https://www.kickstarter.com/projects/460355237/smartibot-the-worlds-first-ai-enabled-cardboard-ro},
lastaccessed={August 28, 2018}
}

@incollection{nishio2007geminoid,
  title={Geminoid: Teleoperated Android of an Existing Person},
  author={Nishio, Shuichi and Ishiguro, Hiroshi and Hagita, Norihiro},
  booktitle={Humanoid robots: new developments},
  year={2007},
  publisher={InTech}
}

@inproceedings{ribeiro2016sera,
  title={The Sera Ecosystem: Socially Expressive Robotics Architecture for Autonomous Human-Robot Interaction},
  author={Ribeiro, Tiago and Pereira, Andr{\'e} and Di Tullio, Eugenio and Paiva, Ana},
  booktitle={AAAI Spring Symposium Series},
  year={2016}
}

@misc{r4amilo,
  author={Robots4Autism},
  title={Meet Milo!},
  year={2015},
  note={Retrieved September 18, 2017 from \url{https://robots4autism.com/milo/}}
}

@phdthesis{setapen2012creating,
  title={Creating Robotic Characters for Long-Term Interaction},
  author={Setapen, Adam Michael},
  year={2012},
  school={Massachusetts Institute of Technology}
}

@misc{singh2016peeqo,
  author={Singh, Abhishek},
  title={Peeqo - The GIF Bot},
  year={2016},
  note={Retrieved September 18, 2017 from \url{https://imgur.com/a/ue4Ax}}
}

@inproceedings{slyper2015mirror,
  title={Mirror Puppeteering: Animating Toy Robots in Front of a Webcam},
  author={Slyper, Ronit and Hoffman, Guy and Shamir, Ariel},
  booktitle={Proceedings of the Ninth International Conference on Tangible, Embedded, and Embodied Interaction},
  pages={241--248},
  year={2015},
  organization={ACM}
}

@online{softbanknao,
  author={SoftBank Robotics},
  title={Nao},
  year={2008},
  url={https://www.ald.softbankrobotics.com/en/robots/nao}
}

@online{hansonsophia,
  author={{Hanson Robotics}},
  title={Sophia},
  year={2016},
  url={https://www.hansonrobotics.com/sophia/}
}

@online{ankivector,
  author={Anki},
  title={Anki Vector},
  year={2018},
  url={https://anki.com/en-us/vector.html},
}


@online{softbankpepper,
  author={SoftBank Robotics},
  title={Pepper, the Humanoid Robot From SoftBank Robotics},
  year={2014},
  url={https://www.softbankrobotics.com/emea/en/robots/pepper}
}

@online{cozmo,
author={Anki Corporation},
title = {Meet Cozmo},
url = {https://www.anki.com/en-us/cozmo},
year = {2017},
lastaccessed = {August 28, 2018}
}

@inproceedings{sun2016psychological,
  title={Psychological Importance of Human Agency: How Self-Assembly Affects User Experience of Robots},
  author={Sun, Yuan and Sundar, S Shyam},
  booktitle={The Eleventh ACM/IEEE International Conference on Human Robot Interaction},
  pages={189--196},
  year={2016},
  organization={IEEE Press}
}

@article{sunspiral2015super,
  title={Super Ball Bot-Structures for Planetary Landing and Exploration, NIAC Phase 2 Final Report},
  author={SunSpiral, Vytas and Agogino, Adrian and Atkinson, David},
  year={2015}
}

@misc{tumroboy,
  author={Technische Universit{\"a}t M{\"u}nchen},
  title={Hello My Name Is Roboy},
  year={2013},
  note={Retrieved September 18, 2017 from \url{http://roboy.org}}
}

@inproceedings{vandevelde2013systems,
  title={Systems Overview of Ono},
  author={Vandevelde, Cesar and Saldien, Jelle and Ciocci, Maria-Cristina and Vanderborght, Bram},
  booktitle={International Conference on Social Robotics},
  pages={311--320},
  year={2013},
  organization={Springer}
}

@inproceedings{wistort2009tofu,
  title={TOFU: A Socially Expressive Robot Character for Child Interaction},
  author={Wistort, Ryan and Breazeal, Cynthia},
  booktitle={Proceedings of the 8th International Conference on Interaction Design and Children},
  pages={292--293},
  year={2009},
  organization={ACM}
}

@article{young2014design,
  title={Design and Evaluation Techniques for Authoring Interactive and Stylistic Behaviors},
  author={Young, James E and Igarashi, Takeo and Sharlin, Ehud and Sakamoto, Daisuke and Allen, Jeffrey},
  journal={ACM Transactions on Interactive Intelligent Systems (TiiS)},
  volume={3},
  number={4},
  pages={23},
  year={2014},
  publisher={ACM}
}


@inproceedings{gielniak2010secondary,
  title={Secondary Action in Robot Motion},
  author={Gielniak, Michael J and Liu, C Karen and Thomaz, Andrea L},
  booktitle={RO-MAN, 2010 IEEE},
  pages={310--315},
  year={2010},
  organization={IEEE}
}

@inproceedings{gielniak2012enhancing,
  title={Enhancing Interaction Through Exaggerated Motion Synthesis},
  author={Gielniak, Michael J and Thomaz, Andrea L},
  booktitle={Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction},
  pages={375--382},
  year={2012},
  organization={ACM}
}

@inproceedings{szafir2014communication,
  title={Communication of Intent in Assistive Free Flyers},
  author={Szafir, Daniel and Mutlu, Bilge and Fong, Terrence},
  booktitle={Proceedings of the 2014 ACM/IEEE international conference on Human-robot interaction},
  pages={358--365},
  year={2014},
  organization={ACM}
}
@inproceedings{van2004animation,
  title={Animation Engine for Believable Interactive User-Interface Robots},
  author={van Breemen, Albert JN},
  booktitle={Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on},
  volume={3},
  pages={2873--2878},
  year={2004},
  organization={IEEE}
}

@article{goodrich07,
  Author = {Goodrich, Michael A. and Schultz, Alan C.},
  Journal = {Foundations and Trends{\textregistered} in Human-Computer Interaction},
  Month = feb,
  Number = {3},
  Pages = {203--275},
  Title = {{Human-Robot Interaction: A Survey}},
  Volume = {1},
  Year = {2007}
}

@inproceedings{kory2016tega,
  title={Tega: A Social Robot},
  author={Kory Westlund, Jacqueline and Lee, Jin Joo and Plummer, Luke and Faridi, Fardad and Gray, Jesse and Berlin, Matt and Quintus-Bosz, Harald and Hartmann, Robert and Hess, Mike and Dyer, Stacy and others},
  booktitle={The Eleventh ACM/IEEE International Conference on Human Robot Interaction},
  pages={561--561},
  year={2016},
  organization={IEEE Press}
}

@incollection{causo2016design,
  title={Design of Robots Used as Education Companion and Tutor},
  author={Causo, Albert and Vo, Giang Truong and Chen, I-Ming and Yeo, Song Huat},
  booktitle={Robotics and Mechatronics},
  pages={75--84},
  year={2016},
  publisher={Springer}
}

@inproceedings{van2005icat,
  title={iCat: An Animated User-Interface Robot With Personality},
  author={van Breemen, Albert and Yan, Xue and Meerbeek, Bernt},
  booktitle={Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems},
  pages={143--144},
  year={2005},
  organization={ACM}
}

@inproceedings{hoffman2012dumb,
  title={Dumb Robots, Smart Phones: A Case Study of Music Listening Companionship},
  author={Hoffman, Guy},
  booktitle={RO-MAN, 2012 IEEE},
  pages={358--363},
  year={2012},
  organization={IEEE}
}

@article{cakmak2010designing,
  title={Designing Interactions for Robot Active Learners},
  author={Cakmak, Maya and Chao, Crystal and Thomaz, Andrea L},
  journal={IEEE Transactions on Autonomous Mental Development},
  volume={2},
  number={2},
  pages={108--118},
  year={2010},
  publisher={IEEE}
}

@book{negroponte1996being,
  title={Being Digital},
  author={Negroponte, Nicholas},
  year={1996},
  publisher={Vintage}
}

@article{negroponte1998beyond,
  title={Beyond Digital},
  author={Negroponte, Nicholas},
  journal={Wired},
  volume={6},
  number={12},
  pages={288},
  year={1998}
}

@article{cascone2000aesthetics,
  title={The Aesthetics of Failure: “Post-Digital” Tendencies in Contemporary Computer Music},
  author={Cascone, Kim},
  journal={Computer Music Journal},
  volume={24},
  number={4},
  pages={12--18},
  year={2000},
  publisher={MIT Press}
}

@article{andrewspost,
  title={Post-Digital Aesthetics and the Return to Modernism, 2000},
  author={Andrews, Ian},
  journal={URL: http://www. ian-andrews. org/texts/postdig. html}
}

@article{andrews2013post,
  title={Post-Digital Aesthetics and the Function of Process.},
  author={Andrews, Ian},
  year={2013},
  publisher={ISEA International}
}

@misc{benjamin1935work,
  title={The Work of Art in the Age of Mechanical Reproduction},
  author={Benjamin, Walter},
  year={1935},
  publisher={Published}
}

@incollection{cramer2015post,
  title={What Is ‘Post-Digital’?},
  author={Cramer, Florian},
  booktitle={Postdigital Aesthetics},
  pages={12--26},
  year={2015},
  publisher={Springer}
}

@inproceedings{kuznetsov2010rise,
  title={Rise of the Expert Amateur: DIY Projects, Communities, and Cultures},
  author={Kuznetsov, Stacey and Paulos, Eric},
  booktitle={Proceedings of the 6th Nordic conference on human-computer interaction: extending boundaries},
  pages={295--304},
  year={2010}
}

@online{dunnerabycritical,
  title = {Critical Design {FAQ}},
  author={Anthony Dunne and Fiona Raby},
  url = {http://dunneandraby.co.uk/content/bydandr/13/0},
}

@online{leesuicidal,
  title = {Suicidal Holder},
  author={Young Suk Lee},
  url={http://www.youngsuklee.com/Suicidal%20object.html},
}

@online{laban,
  title = {Laban Movement Analysis},
  author={Laban/Bartenieff Institute of Movement Studies},
  url = {https://labaninstitute.org/about/laban-movement-analysis/},
}

@online{vergeblossom,
  title = {Someone Please Save This Robot},
  author={Ashley Carman},
  url = {https://www.theverge.com/circuitbreaker/2017/8/29/16222278/blossom-handcrafted-social-robot},
}

@online{suicidal,
  title = {Suicidal Holder 1},
  author={Young Suk Lee},
  url = {https://www.youngsuklee.com/Suicidal%20object.html},
}

@online{aibo,
  title = {Aibo},
  author={Sony},
  url = {https://us.aibo.com/},
}
http://www.sony.net/SonyInfo/News/Press_Archive/199905/99-046/index.html


@online{roomba,
  title = {iRobot Roomba},
  author={iRobot},
  url = {https://www.irobot.com/roomba},
}
https://www.irobot.com/roomba


@online{davinci,
  title = {The Automata of {L}eonardo Da {V}inci},
  author={History-Computer},
  url = {https://history-computer.com/Dreamers/LeonardoAutomata.html},
}

@article{riskin2003defecating,
  title={The Defecating Duck, Or, the Ambiguous Origins of Artificial Life},
  author={Riskin, Jessica},
  journal={Critical Inquiry},
  volume={29},
  number={4},
  pages={599--633},
  year={2003},
  publisher={The University of Chicago Press}
}

@book{dunne2001design,
  title={Design Noir: The Secret Life of Electronic Objects},
  author={Dunne, Anthony and Raby, Fiona},
  year={2001},
  publisher={Springer Science \& Business Media}
}

@inproceedings{bardzell2013critical,
  title={What Is" Critical" About Critical Design?},
  author={Bardzell, Jeffrey and Bardzell, Shaowen},
  booktitle={Proceedings of the SIGCHI conference on human factors in computing systems},
  pages={3297--3306},
  year={2013}
}

@inproceedings{bardzell2012critical,
  title={Critical Design and Critical Theory: The Challenge of Designing for Provocation},
  author={Bardzell, Shaowen and Bardzell, Jeffrey and Forlizzi, Jodi and Zimmerman, John and Antanitis, John},
  booktitle={Proceedings of the Designing Interactive Systems Conference},
  pages={288--297},
  year={2012}
}

@inproceedings{harris2011exploring,
  title={Exploring the Affect of Abstract Motion in Social Human-Robot Interaction},
  author={Harris, John and Sharlin, Ehud},
  booktitle={2011 Ro-Man},
  pages={441--448},
  year={2011},
  organization={IEEE}
}

@inproceedings{forlizzi2007robotic,
  title={How Robotic Products Become Social Products: An Ethnographic Study of Cleaning in the Home},
  author={Forlizzi, Jodi},
  booktitle={2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={129--136},
  year={2007},
  organization={IEEE}
}

@inproceedings{sung2007my,
  title={“My Roomba Is Rambo”: Intimate Home Appliances},
  author={Sung, Ja-Young and Guo, Lan and Grinter, Rebecca E and Christensen, Henrik I},
  booktitle={International conference on ubiquitous computing},
  pages={145--162},
  year={2007},
  organization={Springer}
}

@inproceedings{van2019knock,
  title={Knock on Wood: The Effects of Material Choice on the Perception of Social Robots},
  author={van Waveren, Sanne and Bj{\"o}rklund, Linn{\'e}a and Carter, Elizabeth J and Leite, Iolanda},
  booktitle={International Conference on Social Robotics},
  pages={211--221},
  year={2019},
  organization={Springer}
}

@article{venture2019robot,
  title={Robot Expressive Motions: A Survey of Generation and Evaluation Methods},
  author={Venture, Gentiane and Kuli{\'c}, Dana},
  journal={ACM Transactions on Human-Robot Interaction (THRI)},
  volume={8},
  number={4},
  pages={1--17},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{russell1980circumplex,
  title={A Circumplex Model of Affect.},
  author={Russell, James A},
  journal={Journal of Personality and Social Psychology},
  volume={39},
  number={6},
  pages={1161},
  year={1980},
  publisher={American Psychological Association}
}

@book{norman2013design,
  title={The Design of Everyday Things: Revised and Expanded Edition},
  author={Norman, Don},
  year={2013},
  publisher={Basic books}
}

@book{breazeal2004designing,
  title={Designing Sociable Robots},
  author={Breazeal, Cynthia},
  year={2004},
  publisher={MIT press}
}

@article{triggs2006scissors,
  title={Scissors and Glue: Punk Fanzines and the Creation of a DIY Aesthetic},
  author={Triggs, Teal},
  journal={Journal of Design History},
  volume={19},
  number={1},
  pages={69--83},
  year={2006},
  publisher={Oxford University Press}
}

@article{paulos2012you,
  title={You Amateur!},
  author={Paulos, Eric},
  journal={Interactions},
  volume={19},
  number={1},
  pages={52--57},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@book{shirky2008here,
  title={Here Comes Everybody: The Power of Organizing Without Organizations},
  author={Shirky, Clay},
  year={2008},
  publisher={Penguin}
}

@article{shapin2008scientist,
  title={The Scientist in 2008},
  author={Shapin, Steve},
  journal={Seed Magazine},
  volume={19},
  number={20},
  pages={58--62},
  year={2008}
}

@unpublished{weisser1994keynote,
  title={Building Invisible Interfaces},
  author={Mark Weisser},
  year={1994},
  note={Symposium on User Interface Software and Technology (UIST), Marina del Ray, CA},

}

@inproceedings{you2016cross,
  title={Cross-Modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia},
  author={You, Quanzeng and Luo, Jiebo and Jin, Hailin and Yang, Jianchao},
  booktitle={Proceedings of the Ninth ACM international conference on Web search and data mining},
  pages={13--22},
  year={2016}
}

@inproceedings{wang2017adversarial,
  title={Adversarial Cross-Modal Retrieval},
  author={Wang, Bokun and Yang, Yang and Xu, Xing and Hanjalic, Alan and Shen, Heng Tao},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={154--162},
  year={2017}
}

@article{walter1950imitation,
  title={An Imitation  of Life},
  author={Walter, W Grey},
  journal={Scientific American},
  volume={182},
  number={5},
  pages={42--45},
  year={1950},
  publisher={JSTOR}
}

@inproceedings{dautenhahn1999bringing,
  title={Bringing Up Robots Or The Psychology of Socially Intelligent Robots: From Theory to Implementation},
  author={Dautenhahn, Kerstin and Billard, Aude},
  booktitle={Proceedings of the 3rd Annual Conference on Autonomous Agents},
  pages={366--367},
  year={1999}
}

@article{fong2003survey,
  title={A Survey of Socially Interactive Robots},
  author={Fong, Terrence and Nourbakhsh, Illah and Dautenhahn, Kerstin},
  journal={Robotics and Autonomous Systems},
  volume={42},
  number={3-4},
  pages={143--166},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{brooks1998cog,
  title={The Cog Project: Building a Humanoid Robot},
  author={Brooks, Rodney A and Breazeal, Cynthia and Marjanovi{\'c}, Matthew and Scassellati, Brian and Williamson, Matthew M},
  booktitle={International Workshop on Computation for Metaphors, Analogy, and Agents},
  pages={52--87},
  year={1998},
  organization={Springer}
}

@inproceedings{sakagami2002intelligent,
  title={The Intelligent ASIMO: System Overview and Integration},
  author={Sakagami, Yoshiaki and Watanabe, Ryujin and Aoyama, Chiaki and Matsunaga, Shinichi and Higaki, Nobuo and Fujimura, Kikuo},
  booktitle={IEEE/RSJ international conference on intelligent robots and systems},
  volume={3},
  pages={2478--2483},
  year={2002},
  organization={IEEE}
}

@inproceedings{thibault2018made,
  title={Paper-Made Digital Games. The Poetic of Cardboard From Crayon Physics Deluxe to Nintendo Labo},
  author={Thibault, Mattia},
  year={2018},
  booktitle={Proceedings of the 2018 DiGRA International Conference}
}

@phdthesis{vandeveldethesis,
  author       = {Cesar Vandevelde}, 
  title        = {Study on the Design of DIY Social Robots},
  school       = {Ghent University},
  year         = 2017,
}

@phdthesis{rafflethesis,
  author       = {Hayes Solos Raffle}, 
  title        = {Sculpting Behavior, a Tangible Language for Hands-On Play and Learning},
  school       = {Massachusetts Institute of Technology},
  year         = 2008,
}

@inproceedings{roth2017stabilizing,
  title={Stabilizing Training of Generative Adversarial Networks Through Regularization},
  author={Roth, Kevin and Lucchi, Aurelien and Nowozin, Sebastian and Hofmann, Thomas},
  booktitle={Advances in neural information processing systems},
  pages={2018--2028},
  year={2017}
}

@inproceedings{suguitan2019affective,
  title={Affective Robot Movement Generation Using CycleGANs},
  author={Suguitan, Michael and Bretan, Mason and Hoffman, Guy},
  booktitle={2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={534--535},
  year={2019},
  organization={IEEE}
}

@inproceedings{sandoval2014human,
  title={Human Robot Interaction and Fiction: A Contradiction},
  author={Sandoval, Eduardo Benitez and Mubin, Omar and Obaid, Mohammad},
  booktitle={International Conference on Social Robotics},
  pages={54--63},
  year={2014},
  organization={Springer}
}

@inproceedings{vandevelde2016open,
  title={An Open Platform for the Design of Social Robot Embodiments for Face-To-Face Communication},
  author={Vandevelde, Cesar and Saldien, Jelle},
  booktitle={2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
  pages={287--294},
  year={2016},
  organization={IEEE}
}

@InProceedings{Blukis:19drone-sureal,
  author = "Valts Blukis and Yannick Terme and Eyvind Niklasson and Ross A. Knepper and Yoav Artzi",
  title = "Learning to Map Natural Language Instructions to Physical Quadcopter Control Using Simulated Flight",
  booktitle = "Proceedings of the Conference on Robot Learning",
  year = "2019",
  location = "Osaka, Japan"
}

@article{bretan2016unit,
  title={A Unit Selection Methodology for Music Generation Using Deep Neural Networks},
  author={Bretan, Mason and Weinberg, Gil and Heck, Larry},
  journal={arXiv Preprint arXiv:1612.03789},
  year={2016}
}

@inproceedings{hunt1996unit,
  title={Unit Selection in a Concatenative Speech Synthesis System Using a Large Speech Database},
  author={Hunt, Andrew J and Black, Alan W},
  booktitle={1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings},
  volume={1},
  pages={373--376},
  year={1996},
  organization={IEEE}
}

@book{mcluhan1964understanding,
  title={Understanding Media: The Extensions of Man},
  author={McLuhan, Marshall},
  year={1964},
  publisher={McGraw-Hill}
}

@book{baudrillard1994simulacra,
  title={Simulacra and Simulation},
  author={Baudrillard, Jean},
  year={1994},
  publisher={University of Michigan press}
}

@phdthesis{van2018generating,
  title={Generating Music From Text: Mapping Embeddings to a VAE’s Latent Space},
  author={van der Weerdt, Roderick},
  year={2018},
  school={MSc thesis, Universiteit van Amsterdam}
}

@inproceedings{mubin2016towards,
  title={Towards an Agenda for Sci-Fi Inspired HCI Research},
  author={Mubin, Omar and Obaid, Mohammad and Jordan, Philipp and Alves-Oliveria, Patricia and Eriksson, Thommy and Barendregt, Wolmet and Sjolle, Daniel and Fjeld, Morten and Simoff, Simeon and Billinghurst, Mark},
  booktitle={Proceedings of the 13th International Conference on Advances in Computer Entertainment Technology},
  pages={1--6},
  year={2016}
}

@book{camp2007anime,
  title={Anime Classics Zettai!: 100 Must-See Japanese Animation Masterpieces},
  author={Camp, Brian and Davis, Julie},
  year={2007},
  publisher={Stone Bridge Press, Inc.}
}

@article{kaplan2004afraid,
  title={Who Is Afraid of the Humanoid? Investigating Cultural Differences in the Acceptance of Robots},
  author={Kaplan, Fr{\'e}d{\'e}ric},
  journal={International Journal of Humanoid Robotics},
  volume={1},
  number={03},
  pages={465--480},
  year={2004},
  publisher={World Scientific}
}

@misc{chollet2019measure,
    title={On the Measure of Intelligence},
    author={François Chollet},
    year={2019},
    eprint={1911.01547},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{mcneill2019predicting,
  title={Predicting Human Interpretations of Affect and Valence in a Social Robot},
  author={McNeill, David and Kennington, Casey},
  year={2019}
}

@inproceedings{cederborg2015policy,
  title={Policy Shaping With Human Teachers},
  author={Cederborg, Thomas and Grover, Ishaan and Isbell, Charles L and Thomaz, Andrea L},
  booktitle={Twenty-Fourth International Joint Conference on Artificial Intelligence},
  year={2015}
}

@inproceedings{knox2009interactively,
  title={Interactively Shaping Agents via Human Reinforcement: The TAMER Framework},
  author={Knox, W Bradley and Stone, Peter},
  booktitle={Proceedings of the fifth international conference on Knowledge capture},
  pages={9--16},
  year={2009}
}

@article{han2001internet,
  title={Internet Control Architecture for Internet-Based Personal Robot},
  author={Han, Kuk-Hyun and Kim, Sinn and Kim, Yong-Jae and Kim, Jong-Hwan},
  journal={Autonomous Robots},
  volume={10},
  number={2},
  pages={135--147},
  year={2001},
  publisher={Springer}
}

@inproceedings{luo2000intelligent,
  title={Intelligent Autonomous Mobile Robot Control Through the Internet},
  author={Luo, Ren C and Chen, Tse Min and Yih, Chih-Chen},
  booktitle={ISIE'2000. Proceedings of the 2000 IEEE International Symposium on Industrial Electronics (Cat. No. 00TH8543)},
  volume={1},
  pages={PL6--P11},
  year={2000},
  organization={IEEE}
}

@inproceedings{suzuki1996teleoperation,
  title={Teleoperation of Multiple Robots Through the Internet},
  author={Suzuki, Tsuyoshi and Fujii, Teruo and Yokota, Kazutaka and Asama, Hajime and Kaetsu, Hayato and Endo, Isao},
  booktitle={Proceedings 5th IEEE International Workshop on Robot and Human Communication. RO-MAN'96 TSUKUBA},
  pages={84--89},
  year={1996},
  organization={IEEE}
}

@inproceedings{lucas2018getting,
  title={Getting to Know Each Other: The Role of Social Dialogue in Recovery From Errors in Social Robots},
  author={Lucas, Gale M and Boberg, Jill and Traum, David and Artstein, Ron and Gratch, Jonathan and Gainer, Alesia and Johnson, Emmanuel and Leuski, Anton and Nakano, Mikio},
  booktitle={Proceedings of the 2018 acm/ieee international conference on human-robot interaction},
  pages={344--351},
  year={2018}
}

@article{giger2019humanization,
  title={Humanization of Robots: Is It Really Such a Good Idea?},
  author={Giger, Jean-Christophe and Pi{\c{c}}arra, Nuno and Alves-Oliveira, Patrícia and Oliveira, Raquel and Arriaga, Patrícia},
  journal={Human Behavior and Emerging Technologies},
  volume={1},
  number={2},
  pages={111--123},
  year={2019},
  publisher={Wiley Online Library}
}

@article{persson2000anthropomorphism,
  title={Anthropomorphism--A Multi-Layered Phenomenon},
  author={Persson, Per and Laaksolahti, Jarmo and L{\"o}nnqvist, Peter},
  journal={Proc. Socially Intelligent Agents--The Human in the Loop, AAAI Press, Technical Report FS-00-04},
  pages={131--135},
  year={2000}
}

@article{mori1970uncanny,
  title={The Phenomenon of the Valley of Eerieness (The Uncanny Valley)},
  author={Mori, Masahiro},
  journal={Energy},
  volume={7},
  number={4},
  pages={33--35},
  year={1970}
}
@article{mori2012uncanny,
  title={The uncanny valley [from the field]},
  author={Mori, Masahiro and MacDorman, Karl F and Kageki, Norri},
  journal={IEEE Robotics \& Automation Magazine},
  volume={19},
  number={2},
  pages={98--100},
  year={2012},
  publisher={IEEE}
}

@article{parkintelligent,
  title={Intelligent Social Robots in the Wild: A Qualitative Study on Deploying Intelligent Social Robots With Growing Features to Real Home Environments},
  author={Park, Chan Mi and Jeong, Yuin and Jeong, Kwangmin and Lee, Hae-Sung and Lee, Jeehang and Kim, Jinwoo}
}

@online{stelarcstickman,
  title = {Reclining Stickman},
  author={Stelarc},
  url = {https://recliningstickman.stelarc.org/},
  year={2020},
}

@book{shelley1818frankenstein,
  title={Frankenstein; Or, the Modern Prometheus},
  author={Shelley, Mary Wollstonecraft},
  year={1818},
}

@film{rur,
  title={{Rossum's Universal Robots}},
  author="Karel \v{C}apek",
  year={1920},
}

@film{metropolis,
  title={Metropolis},
  author={Fritz Lang},
  year={1927},
}

@misc{astroboy,
  title={{Astro Boy}},
  author={Tezuka Osamu},
  year={1963},
}

@film{bladerunner,
  title={{Blade Runner}},
  author={Ridley Scott},
  year={1982},
}
@film{2001,
  title={{2001: A Space Odyssey}},
  author={Stanley Kubrick},
  year={1968},
}
@film{walle,
  title={{WALL-E}},
  author={Andrew Stanton},
  year={2008},
}
@film{exmachina,
  title={{Ex-Machina}},
  author={Alex Garland},
  year={2014},
}

@misc{evangelion,
  title={{Neon Genesis Evangelion}},
  author={Anno, Hideaki},
  year={1995},
}

@misc{anno1996human,
title={{At Least, Be Human}},
volume={1}, number={22},
journal={Neon Genesis Evangelion},
author={Anno, Hideaki},
year={1996},
month={Feb}}

@misc{gundam,
  title={{Mobile Suit Gundam}},
  author={Tomino, Yoshiyuki},
  year={1979},
}

@misc{mazinger,
  title={Mazinger},
  author={Nagai, Go},
  year={1972},
}

@misc{tetsujin,
  title={Tetsujin 28-Go},
  author={Yokoyama, Mitsuteru},
  year={1963},
}

@article{richardson2012modern,
  title={The Modern Robot and the Postmodern Cyborg: The Post-Human as an Image of Anxiety},
  author={Richardson, Leanna},
  journal={Emergence: A Journal of Undergraduate Literary Criticism and Creative Research},
  volume={3},
  year={2012}
}

@online{radios,
  title={Amateur Radio History},
  author={Electronics Notes},
  url={https://www.electronics-notes.com/articles/history/amateur-ham-radio/history.php},
}

@inproceedings{knight2016laban,
  title={Laban Head-Motions Convey Robot State: A Call for Robot Body Language},
  author={Knight, Heather and Simmons, Reid},
  booktitle={2016 IEEE international conference on robotics and automation (ICRA)},
  pages={2881--2888},
  year={2016},
  organization={IEEE}
}

@inproceedings{rett2007human,
  title={Human-Robot Interface With Anticipatory Characteristics Based on Laban Movement Analysis and Bayesian Models},
  author={Rett, Jorg and Dias, Jorge},
  booktitle={2007 IEEE 10th international conference on rehabilitation robotics},
  pages={257--268},
  year={2007},
  organization={IEEE}
}

@inproceedings{masuda2009emotion,
  title={Emotion Detection From Body Motion of Human Form Robot Based on Laban Movement Analysis},
  author={Masuda, Megumi and Kato, Shohei and Itoh, Hidenori},
  booktitle={International Conference on Principles and Practice of Multi-Agent Systems},
  pages={322--334},
  year={2009},
  organization={Springer}
}

@inproceedings{van2004bringing,
  title={Bringing Robots to Life: Applying Principles of Animation to Robots},
  author={Van Breemen, AJN},
  booktitle={Proceedings of Shapping Human-Robot Interaction workshop held at CHI},
  volume={2004},
  pages={143--144},
  year={2004},
  organization={Citeseer}
}

@book{thomas1981illusion,
  title={The Illusion of Life: Disney Animation},
  author={Thomas, Frank and Johnston, Ollie and Thomas, Frank},
  year={1981},
}

@inproceedings{takayama2011expressing,
  title={Expressing Thought: Improving Robot Readability With Animation Principles},
  author={Takayama, Leila and Dooley, Doug and Ju, Wendy},
  booktitle={Proceedings of the 6th international conference on Human-robot interaction},
  pages={69--76},
  year={2011}
}

@article{bretan2015emotionally,
  title={Emotionally Expressive Dynamic Physical Behaviors in Robots},
  author={Bretan, Mason and Hoffman, Guy and Weinberg, Gil},
  journal={International Journal of Human-Computer Studies},
  volume={78},
  pages={1--16},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{suguitan2020moveae,
  title={MoveAE: Modifying Affective Robot Movements Using Classifying Variational Autoencoders},
  author={Suguitan, Michael and Gomez, Randy and Hoffman, Guy},
  booktitle={Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={481--489},
  year={2020},
  doi={10.1145/3319502.3374807}
}

@misc{flask,
  author={Armin Ronacher},
  year={2010},
}

@inproceedings{atzeni2018deep,
  title={Deep Learning and Sentiment Analysis for Human-Robot Interaction},
  author={Atzeni, Mattia and Recupero, Diego Reforgiato},
  booktitle={European Semantic Web Conference},
  pages={14--18},
  year={2018},
  organization={Springer}
}

@article{liu2020deep,
  title={Deep Learning for Generic Object Detection: A Survey},
  author={Liu, Li and Ouyang, Wanli and Wang, Xiaogang and Fieguth, Paul and Chen, Jie and Liu, Xinwang and Pietik{\"a}inen, Matti},
  journal={International Journal of Computer Vision},
  volume={128},
  number={2},
  pages={261--318},
  year={2020},
  publisher={Springer}
}

@inproceedings{marmpena2019generating,
  title={Generating Robotic Emotional Body Language With Variational Autoencoders},
  author={Marmpena, Mina and Lim, Angelica and Dahl, Torbj{\o}rn S and Hemion, Nikolas},
  booktitle={2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)},
  pages={545--551},
  year={2019},
  organization={IEEE}
}

@inproceedings{alves2020yolo,
  title={YOLO-Your Own Living Object},
  author={Alves-Oliveira, Patrícia and Arriaga, Patrícia and Paiva, Ana and Hoffman, Guy},
  booktitle={Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={638--638},
  year={2020}
}

@article{dhariwal2020jukebox,
  title={Jukebox: A Generative Model for Music},
  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  journal={arXiv Preprint arXiv:2005.00341},
  year={2020}
}

@misc{lobe,
  title={Lobe},
  author={Lobe AI},
  url={lobe.ai}
}

@misc{tfplayground,
  title={A Neural Network Playground},
  author={Google},
  url={playground.tensorflow.org},
}

@misc{humanize,
  title={Humanize},
  author={Merriam-Webster},
  url={https://www.merriam-webster.com/dictionary/humanize}
}
@misc{affinity,
  title={Affinity},
  author={Merriam-Webster},
  url={https://www.merriam-webster.com/dictionary/affinity}
}
@misc{medium,
  title={Medium},
  author={Merriam-Webster},
  url={https://www.merriam-webster.com/dictionary/medium}
}
@misc{communication,
  title={Communication},
  author={Merriam-Webster},
  url={https://www.merriam-webster.com/dictionary/communication}
}
@misc{accessibility,
  title={Accessibility},
  author={Merriam-Webster},
  url={https://www.merriam-webster.com/dictionary/accessibility}
}
@misc{robot,
  title={Robot},
  author={Merriam-Webster},
  url={https://www.merriam-webster.com/dictionary/robot}
}



@inproceedings{dautenhahn1999bringing,
  title={Bringing up robots or—the psychology of socially intelligent robots: From theory to implementation},
  author={Dautenhahn, Kerstin and Billard, Aude},
  booktitle={Proceedings of the third annual conference on Autonomous Agents},
  pages={366--367},
  year={1999}
}

@article{duffy2003anthropomorphism,
  title={Anthropomorphism and the Social Robot},
  author={Duffy, Brian R},
  journal={Robotics and Autonomous Systems},
  volume={42},
  number={3-4},
  pages={177--190},
  year={2003},
  publisher={Elsevier}
}

@article{sequeira2018can,
  title={Can Social Robots Make Societies More Human?},
  author={Sequeira, Jo{\~a}o Silva},
  journal={Information},
  volume={9},
  number={12},
  pages={295},
  year={2018},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{huron2009facial,
  title={Facial expression and vocal pitch height: Evidence of an intermodal association},
  author={Huron, David and Dahl, Sofia and Johnson, Randolph},
  year={2009},
  publisher={Empirical Musicology Review}
}

@online{kim2017work,
  title={Let Robots Handle Your Emotional Burnout at Work},
  author={Meeri Kim},
  year={2017},
  url={https://howwegettonext.com/let-robots-handle-your-emotional-burnout-at-work-e09babbe81e8#.4933cuqsj}
  
}

@phdthesis{simbelis2018humanizing,
  title={Humanizing Technology Through Post-Digital Art},
  author={Simbelis, Vygandas Vegas},
  year={2018},
  school={KTH Royal Institute of Technology}
}

@book{alexenberg2011future,
  title={The future of art in a postdigital age: from Hellenistic to Hebraic consciousness},
  author={Alexenberg, Melvin L},
  year={2011},
  publisher={Intellect Books}
}


@article{page2014skeuomorphism,
  title={Skeuomorphism or flat design: future directions in mobile device User Interface (UI) design education.},
  author={Page, Tom},
  journal={IJMLO},
  volume={8},
  number={2},
  pages={130--142},
  year={2014}
}

@article{naumov2019deep,
  title={Deep learning recommendation model for personalization and recommendation systems},
  author={Naumov, Maxim and Mudigere, Dheevatsa and Shi, Hao-Jun Michael and Huang, Jianyu and Sundaraman, Narayanan and Park, Jongsoo and Wang, Xiaodong and Gupta, Udit and Wu, Carole-Jean and Azzolini, Alisson G and others},
  journal={arXiv preprint arXiv:1906.00091},
  year={2019}
}

@article{schneider2019mass,
  title={Mass Personalization of Deep Learning},
  author={Schneider, Johannes and Vlachos, Michail},
  journal={arXiv preprint arXiv:1909.02803},
  year={2019}
}


@online{rovingtypist,
  title = {Roving Typist},
  author={Hermelin, C.D.},
  url = {https://rovingtypist.com/},
  year={2012},
}

@InProceedings{mandlekar2018roboturk,
  title =    {{RoboTurk}: A Crowdsourcing Platform for Robotic Skill Learning through Imitation},
  author =       {Mandlekar, Ajay and Zhu, Yuke and Garg, Animesh and Booher, Jonathan and Spero, Max and Tung, Albert and Gao, Julian and Emmons, John and Gupta, Anchit and Orbay, Emre and Savarese, Silvio and Fei-Fei, Li},
  booktitle =    {Proceedings of The 2nd Conference on Robot Learning},
  pages =    {879--893},
  year =   {2018},
  volume =   {87},
  series =   {Proceedings of Machine Learning Research},
  address =    {},
  month =    {29--31 Oct},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v87/mandlekar18a/mandlekar18a.pdf},
  url =    {http://proceedings.mlr.press/v87/mandlekar18a.html},
  abstract =   {Imitation Learning has empowered recent advances in learning robotic manipulation tasks by addressing shortcomings of Reinforcement Learning such as exploration and reward specification. However, research in this area has been limited to modest-sized datasets due to the difficulty of collecting large quantities of task demonstrations through existing mechanisms. This work introduces ROBO-TURK to address this challenge. ROBOTURK is a crowdsourcing platform for high quality 6-DoF trajectory based teleoperation through the use of widely available mobile devices (e.g. iPhone). We evaluate ROBOTURK on three manipulation tasks of varying timescales (15-120s) and observe that our user interface is statistically similar to special purpose hardware such as virtual reality controllers in terms of task completion times. Furthermore, we observe that poor network conditions, such as low bandwidth and high delay links, do not substantially affect the remote usersâ ability to perform task demonstrations successfully on ROBOTURK. Lastly, we demonstrate the efficacy of ROBOTURK through the collection of a pilot dataset; using ROBOTURK, we collected 137.5 hours of manipulation data from remote workers, amounting to over 2200 successful task demonstrations in 22 hours of total system usage. We show that the data obtained through ROBOTURK enables policy learning on multi-step manipulation tasks with sparse rewards and that using larger quantities of demonstrations during policy learning provides benefits in terms of both learning consistency and final performance. For additional results, videos, and to download our pilot dataset, visit roboturk.stanford.edu }
}


@INPROCEEDINGS{muller2016panovc,
  author={J. {M\"{u}ller} and T. {Langlotz} and H. {Regenbrecht}},
  booktitle={2016 IEEE International Conference on Pervasive Computing and Communications}, 
  title={{PanoVC}: Pervasive telepresence using mobile phones}, 
  year={2016},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/PERCOM.2016.7456508}}

@misc{tang2017collaboration,
  title = {Collaboration in 360${}^\circ$ Videochat: Challenges and Opportunities},
  author = {Tang, Anthony and Fakourfar, Omid and Neustaedter, Carman and Bateman, Scott},
  year = {2017},
  URL = {https://prism.ucalgary.ca/handle/1880/51950},
  doi = {10.11575/PRISM/10182},
  publisher = {PRISM}
}

@inproceedings{sakashita2017puppet,
author = {Sakashita, Mose and Minagawa, Tatsuya and Koike, Amy and Suzuki, Ippei and Kawahara, Keisuke and Ochiai, Yoichi},
title = {You as a Puppet: Evaluation of Telepresence User Interface for Puppetry},
year = {2017},
isbn = {9781450349819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126594.3126608},
doi = {10.1145/3126594.3126608},
abstract = {We propose an immersive telepresence system for puppetry that transmits a human performer's body and facial movements into a puppet with audiovisual feedback to the performer. The cameras carried in place of puppet's eyes stream live video to the HMD worn by the performer, so that performers can see the images from the puppet's eyes with their own eyes and have a visual understanding of the puppet's ambience. In conventional methods to manipulate a puppet (a hand-puppet, a string-puppet, and a rod-puppet), there is a need to practice manipulating puppets, and there is difficulty carrying out interactions with the audience. Moreover, puppeteers must be positioned exactly where the puppet is. The proposed system addresses these issues by enabling a human performer to manipulate the puppet remotely using his or her body and facial movements. We conducted several user studies with both beginners and professional puppeteers. The results show that, unlike the conventional method, the proposed system facilitates the manipulation of puppets especially for beginners. Moreover, this system allows performers to enjoy puppetry and fascinate audiences.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
pages = {217-228},
numpages = {12},
keywords = {puppet, head-mounted display, telepresence, animatronics},
location = {Qu\'{e}bec City, QC, Canada},
series = {UIST '17}
}

@inproceedings{jeong2018fribo,
author = {Jeong, Kwangmin and Sung, Jihyun and Lee, Hae-Sung and Kim, Aram and Kim, Hyemi and Park, Chanmi and Jeong, Yuin and Lee, JeeHang and Kim, Jinwoo},
title = {Fribo: A Social Networking Robot for Increasing Social Connectedness through Sharing Daily Home Activities from Living Noise Data},
year = {2018},
isbn = {9781450349536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3171221.3171254},
doi = {10.1145/3171221.3171254},
abstract = {The rapid increase in the number of young adults living alone gives rise to a demand for the resolution of social isolation problems. Social robot technologies play a substantial role for this purpose. However, existing technologies try to solve the problem only through one-to-one interaction with robots, which in turn fails to utilize the real-world social relationships. Privacy concern is an additional issue since most social robots rely on the visual information for the interactions. To this end, we propose 'Fribo', auditory information centered social robot that recognizes user's activity by analyzing occupants' living noise and shares the activity information with close friends. A four-week field study with the first prototype of Fribo confirms that activity sharing through the use of anonymized living noise promises a virtual cohabiting experience that triggers more frequent real-world social interactions with less feeling of privacy intrusion. Based on this finding and the further qualitative analysis, we suggest a design principle of sound-based social networking robots and its associated new interactions, then present the second prototype of Fribo inspired by the implications from the field study.},
booktitle = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {114-122},
numpages = {9},
keywords = {sound recognition, social connectivity, one-person households, living noise, field study.},
location = {Chicago, IL, USA},
series = {HRI '18}
}


@book{goldberg2001garden,
  title={The Robot in the Garden: Telerobotics and Telepistemology in the Age of the Internet},
  author={Goldberg, Ken},
  year={2001},
  publisher={MIT Press},
}

@inproceedings{stiehl2009huggable,
author = {Stiehl, Walter Dan and Lee, Jun Ki and Breazeal, Cynthia and Nalin, Marco and Morandi, Angelica and Sanna, Alberto},
title = {The Huggable: A Platform for Research in Robotic Companions for Pediatric Care},
year = {2009},
isbn = {9781605583952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1551788.1551872},
doi = {10.1145/1551788.1551872},
abstract = {Robotic companions offer a unique combination of embodiment and computation which open many new interesting opportunities in the field of pediatric care. As these new technologies are developed, we must consider the central research questions of how such systems should be designed and what the appropriate applications for such systems are. In this paper we present the Huggable, a robotic companion in the form factor of a teddy bear and outline a series of studies we are planning to run using the Huggable in a pediatric care unit.},
booktitle = {Proceedings of the 8th International Conference on Interaction Design and Children},
pages = {317-320},
numpages = {4},
keywords = {robotic companion, pediatrics, personal robot},
location = {Como, Italy},
series = {IDC '09}
}

@inproceedings{gomez2020robomoji,
  title={Emoji to Robomoji: Exploring Affective Telepresence Through Haru},
  author={Gomez, Randy and Szapiro, Deborah and Merino, Luis and Brock, Heike and Nakamura, Keisuke and Sabanovic, Selma},
  booktitle={International Conference on Social Robotics},
  pages={652--663},
  year={2020},
  organization={Springer},
  doi={10.1007/978-3-030-62056-1_54}
}

@inproceedings{strong1996feather,
  title={Feather, Scent and Shaker: Supporting Simple Intimacy},
  author={Strong, Rob and Gaver, Bill},
  booktitle={Proceedings of CSCW},
  volume={96},
  number={96},
  pages={29--30},
  year={1996}
}

@inproceedings{sirkin2012tele,
author = {Sirkin, David and Ju, Wendy},
title = {Consistency in Physical and On-Screen Action Improves Perceptions of Telepresence Robots},
year = {2012},
isbn = {9781450310635},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2157689.2157699},
doi = {10.1145/2157689.2157699},
abstract = {Does augmented movement capability improve people's experiences with telepresent meeting participants? We performed two web-based studies featuring videos of a telepresence robot. In the first study (N=164), participants observed clips of typical conversational gestures performed a) on a stationary screen only, b) with an actuated screen moving in physical space, or c) both on-screen and in-space. In the second study (N=103), participants viewed scenario videos depicting two people interacting with a remote collaborator through a telepresence robot, whose distant actions were a) visible on the screen only, or b) accompanied by local physical motion. These studies suggest that synchronized on-screen and in-space gestures significantly improved viewers' interpretation of the action compared to on-screen or in-space gestures alone, and that in-space gestures positively influenced perceptions of both local and remote participants.},
booktitle = {Proceedings of the Seventh Annual ACM/IEEE International Conference on Human-Robot Interaction},
pages = {57-64},
numpages = {8},
keywords = {embodied interaction, telepresence robotics, videoconferencing},
location = {Boston, Massachusetts, USA},
series = {HRI '12}
}

@inproceedings{denisova2015first,
author = {Denisova, Alena and Cairns, Paul},
title = {First Person vs. Third Person Perspective in Digital Games: Do Player Preferences Affect Immersion?},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702256},
doi = {10.1145/2702123.2702256},
abstract = {Contemporary digital game developers offer a variety of games for the diverse tastes of their customers. Although the gaming experience often depends on one's preferences, the same may not apply to the level of their immersion. It has been argued whether the player perspective can influence the level of player's involvement with the game. The aim of this study was to research whether interacting with a game in first person perspective is more immersive than playing in the third person point of view (POV). The set up to test the theory involved participants playing a role-playing game in either mode, naming their preferred perspective, and subjectively evaluating their immersive experience. The results showed that people were more immersed in the game play when viewing the game world through the eyes of the character, regardless of their preferred perspectives.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {145-148},
numpages = {4},
keywords = {digital games, player experience, immersion, player perspective, camera point of view},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}



@ARTICLE{gorisse2017first,
  
AUTHOR={Gorisse, Geoffrey and Christmann, Olivier and Amato, Etienne Armand and Richir, Simon},   
   
TITLE={First- and Third-Person Perspectives in Immersive Virtual Environments: Presence and Performance Analysis of Embodied Users},      
  
JOURNAL={Frontiers in Robotics and AI},      
  
VOLUME={4},      

PAGES={33},     
  
YEAR={2017},      
    
URL={https://www.frontiersin.org/article/10.3389/frobt.2017.00033},       
  
DOI={10.3389/frobt.2017.00033},      
  
ISSN={2296-9144},   
   
ABSTRACT={Current design of virtual reality (VR) applications relies essentially on the transposition of users’ viewpoint in first-person perspective (1PP). Within this context, our research aims to compare the impact and the potentialities enabled via the integration of the third-person perspective (3PP) in immersive virtual environments (IVE). Our empirical study is conducted in order to assess the sense of presence, the sense of embodiment, and performance of users confronted with a series of tasks presenting a case of potential use for the video game industry. Our results do not reveal significant differences concerning the sense of spatial presence with either point of view. Nonetheless, they provide evidence confirming the relevance of using the first-person perspective to induce a sense of embodiment toward a virtual body, especially in terms of self-location and ownership. However, no significant differences were observed concerning the sense of agency. Concerning users’ performance, our results demonstrate that the first-person perspective enables more accurate interactions, while the third-person perspective provides better space awareness.}
}

@article{galvan2017characterizing,
    author = {Galvan Debarba, Henrique AND Bovet, Sidney AND Salomon, Roy AND Blanke, Olaf AND Herbelin, Bruno AND Boulic, Ronan},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Characterizing First and Third Person Viewpoints and Their Alternation for Embodied Interaction in Virtual Reality},
    year = {2017},
    month = {12},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0190109},
    pages = {1-19},
    abstract = {Empirical research on the bodily self has shown that the body representation is malleable, and prone to manipulation when conflicting sensory stimuli are presented. Using Virtual Reality (VR) we assessed the effects of manipulating multisensory feedback (full body control and visuo-tactile congruence) and visual perspective (first and third person perspective) on the sense of embodying a virtual body that was exposed to a virtual threat. We also investigated how subjects behave when the possibility of alternating between first and third person perspective at will was presented. Our results support that illusory ownership of a virtual body can be achieved in both first and third person perspectives under congruent visuo-motor-tactile condition. However, subjective body ownership and reaction to threat were generally stronger for first person perspective and alternating condition than for third person perspective. This suggests that the possibility of alternating perspective is compatible with a strong sense of embodiment, which is meaningful for the design of new embodied VR experiences.},
    number = {12},
    doi = {10.1371/journal.pone.0190109}
}







@article{kilteni2012vrsense,
author = {Kilteni, Konstantina and Groten, Raphaela and Slater, Mel},
title = {The Sense of Embodiment in Virtual Reality},
journal = {Presence: Teleoperators and Virtual Environments},
volume = {21},
number = {4},
pages = {373-387},
year = {2012},


URL = { 
        https://doi.org/10.1162/PRES_a_00124
    
},
,
    abstract = { Abstract What does it feel like to own, to control, and to be inside a body? The multidimensional nature of this experience together with the continuous presence of one's biological body, render both theoretical and experimental approaches problematic. Nevertheless, exploitation of immersive virtual reality has allowed a reframing of this question to whether it is possible to experience the same sensations towards a virtual body inside an immersive virtual environment as toward the biological body, and if so, to what extent. The current paper addresses these issues by referring to the Sense of Embodiment (SoE). Due to the conceptual confusion around this sense, we provide a working definition which states that SoE consists of three subcomponents: the sense of self-location, the sense of agency, and the sense of body ownership. Under this proposed structure, measures and experimental manipulations reported in the literature are reviewed and related challenges are outlined. Finally, future experimental studies are proposed to overcome those challenges, toward deepening the concept of SoE and enhancing it in virtual applications. }
}

@misc{mandlekar2019roboturk,
      title={Scaling Robot Supervision to Hundreds of Hours with {RoboTurk}: Robotic Manipulation Dataset through Human Reasoning and Dexterity}, 
      author={Ajay Mandlekar and Jonathan Booher and Max Spero and Albert Tung and Anchit Gupta and Yuke Zhu and Animesh Garg and Silvio Savarese and Li Fei-Fei},
      year={2019},
      eprint={1911.04052},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}


@INPROCEEDINGS{adalgeirsson2010mebot,
  author={Adalgeirsson, Sigurdur and Breazeal, Cynthia},
  booktitle={2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={MeBot: A Robotic Platform for Socially Embodied Telepresence}, 
  year={2010},
  volume={},
  number={},
  pages={15-22},
  doi={10.1109/HRI.2010.5453272}}

@ARTICLE{young2019immersive,
  author={Young, Jacob and Langlotz, Tobias and Cook, Matthew and Mills, Steven and Regenbrecht, Holger},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Immersive Telepresence and Remote Collaboration using Mobile and Wearable Devices}, 
  year={2019},
  volume={25},
  number={5},
  pages={1908-1918},
  doi={10.1109/TVCG.2019.2898737}}


@inproceedings{lombard2009measuring,
  title={Measuring Presence: The Temple Presence Inventory},
  author={Lombard, Matthew, and Ditton, Theresa},
}

@inproceedings{rakita2020latency,
author = {Rakita, Daniel and Mutlu, Bilge and Gleicher, Michael},
title = {Effects of Onset Latency and Robot Speed Delays on Mimicry-Control Teleoperation},
year = {2020},
isbn = {9781450367462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319502.3374838},
doi = {10.1145/3319502.3374838},
abstract = {In this paper, we study the effects of delays in a mimicry-control robot teleoperation interface which involves a user moving their arms to directly show the robot how to move and the robot follows in real time. Unlike prior work considering delays in other teleoperation systems, we consider delays due to robot slowness in addition to latency in the onset of movement commands. We present a human-subjects study that shows how different amounts and types of delays have different effects on task performance. We compare the movements under different delays to reveal the strategies that operators use to adapt to delay conditions and to explain performance differences. Our results show that users can quickly develop strategies to adapt to slowness delays but not onset latency delays. We discuss the implications of our results for the future development of methods designed to mitigate the effects of delays.},
booktitle = {Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {519-527},
numpages = {9},
keywords = {motion retargeting, teleoperation, latency analysis},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}

@inproceedings{tanaka2014comparing,
author="Tanaka, Kazuaki
and Nakanishi, Hideyuki
and Ishiguro, Hiroshi",
editor="Yuizono, Takaya
and Zurita, Gustavo
and Baloian, Nelson
and Inoue, Tomoo
and Ogata, Hiroaki",
title="Comparing Video, Avatar, and Robot Mediated Communication: Pros and Cons of Embodiment",
booktitle="Collaboration Technologies and Social Computing",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="96--110",
abstract="In recent years, studies have begun on robot conferencing as a new telecommunication medium. In robot conferencing, people talk with a remote conversation partner through teleoperated robots which present the bodily motions of the partner with a physical embodiment. However, the effects of physical embodiment on distant communication had not yet been demonstrated. In this study, to find the effects, we conducted an experiment in which subjects talked with a partner through robots and various existing communication media (e.g. voice, avatar and video chats). As a result, we found that the physical embodiment enhanced social telepresence, i.e., the sense of resembling face-to-face interaction. Furthermore, the result implied that physical embodiment built the sense of tension as in the case of a first face-to-face meeting.",
isbn="978-3-662-44651-5"
}


@inproceedings{ainasoja2019smartphone,
  title={Smartphone Teleoperation for Self-balancing Telepresence Robots.},
  author={Ainasoja, Antti E. and Pertuz, Said and K{\"a}m{\"a}r{\"a}inen, Joni-Kristian},
  booktitle={VISIGRAPP (4: VISAPP)},
  pages={561--568},
  year={2019},
  doi={10.5220/0007406405610568}
}

@article{jonggil2018sprint,
  title={SPRinT: A Mixed Approach to a Hand-Held Robot Interface for Telepresence},
  author={Jonggil, Ahn and Kim, Gerard Jounghyun},
  journal={International Journal of Social Robotics},
  volume={10},
  number={4},
  pages={537--552},
  year={2018},
  publisher={Springer Nature BV},
  doi={10.1007/s12369-017-0463-2}
}

@misc{murphy2020covid,
      title={Applications of Robots for {COVID-19} Response}, 
      author={Robin R. Murphy and Vignesh Babu Manjunath Gandudi and Justin Adams},
      year={2020},
      eprint={2008.06976},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{lau2020remotehri,
  title={A JavaScript Framework for Crowdsourced Human-Robot Interaction Experiments: RemoteHRI},
  author={Lau, Finley and Gopinath, Deepak and Argall, Brenna D.},
  booktitle={Association for the Advancement of Artificial Intelligence},
  year={2020}
}


@online{deviceorientation,
author={W3C},
title = {DeviceOrientation Event Specification},
url = {https://www.w3.org/TR/orientation-event/},
year={2019}
}

@online{deviceorientation,
author={W3C},
title = {DeviceOrientation Event Specification},
url = {https://www.w3.org/TR/orientation-event/},
year={2019}
}


@article{feil2020covid,
author = {Feil-Seifer, David and Haring, Kerstin S. and Rossi, Silvia and Wagner, Alan R. and Williams, Tom},
title = {Where to Next? The Impact of COVID-19 on Human-Robot Interaction Research},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
url = {https://doi.org/10.1145/3405450},
doi = {10.1145/3405450},
abstract = {The COVID-19 pandemic will have a profound and long-lasting impact on the entire scientific endeavor. Scientists already are adapting research programs to adapt to changes in what is prioritized—and what is possible; educators are changing the way that the next generation of researchers are trained, and flagship conferences in many fields are being cancelled, postponed, and fundamentally transformed.These broad-reaching changes are particularly impactful to human-oriented domains such as human-robot interaction (HRI). Because in-person human-subject experiments can take a year or more to conduct, the research we will see published in the field in the immediate future may appear to be “business as usual,” with accounts of laboratory studies with large numbers of in-person participants. The research currently being performed, however, is of course a different story entirely. Studies that were under way when the current crisis began will be truncated, resulting either in work that cannot be published or in work whose true impact is difficult to accurately assess. Yet HRI research performed in the coming years will be changed in fundamentally different ways; the inability to perform—or expect future performance of—in-person human subjects research, especially research involving tactile or multiparty interaction, will change both the dominant methodological techniques employed by HRI researchers and the very research questions that the field chooses to—and is able to—address.These challenges demand that HRI researchers identify precisely how the field can maintain research quality and impact while the ability to conduct human-subject studies is severely impaired for an undetermined amount of time. A natural inclination may be simply to wait the crisis out in the hope of a speedy return to normalcy; however, in this article, we argue that the community can also take this opportunity to reevaluate and refocus how research in this field is conducted and how students are mentored in ways that will yield benefits for years to come after the current crisis has ended.},
journal = {J. Hum.-Robot Interact.},
month = jun,
articleno = {1},
numpages = {7},
keywords = {{COVID-19} impact, Human-robot interaction, research}
}


@article{grollman2018content,
  Abstract = {Robot personalities are today painstakingly hand-crafted by content creators who specify motions, facial animations, and scripts in order to convey a unifying sense of character via interactions. While user preferences can be incorporated by modifying these behaviors to some degree, such as by inserting a user's name or preference into a script, by and large new content must be continually created in order to maintain the robot's `freshness.'This content treadmill represents a long tail of work that diverts resources away from improving the underlying capabilities of the robot. Here we outline a novel method for defining robot personality that allows each robot to have and express a unique personality through varying behaviors that reduces the need for content creation.},
  Author = {Grollman, Daniel H. },
  Da = {2018/04/01},
  Date-Added = {2021-02-14 04:40:40 +0000},
  Date-Modified = {2021-02-14 04:40:40 +0000},
  Doi = {10.1007/s12369-017-0451-6},
  Id = {Grollman2018},
  Isbn = {1875-4805},
  Journal = {International Journal of Social Robotics},
  Number = {2},
  Pages = {225--234},
  Title = {Avoiding the Content Treadmill for Robot Personalities},
  Ty = {JOUR},
  Url = {https://doi.org/10.1007/s12369-017-0451-6},
  Volume = {10},
  Year = {2018},
  Bdsk-Url-1 = {https://doi.org/10.1007/s12369-017-0451-6},
  Bdsk-Url-2 = {http://dx.doi.org/10.1007/s12369-017-0451-6}}

@inproceedings{youarenottherobot2021suguitan,
author = {Suguitan, Michael and Hoffman, Guy},
title = {You Are (Not) The Robot: Variable Perspective Motion Control of a Social Telepresence Robot},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451571},
doi = {10.1145/3411763.3451571},
abstract = { COVID-19 has dramatically limited opportunities for in-person human-robot interaction
research and shifted focus towards remote technologies such as telepresence robots.
Telepresence robots enable rich communication and agency through their physical presence
and controllability, but their screen-oriented designs and button-centric controls
abstract users away from their own physicality. In this demonstration, we present
a telepresence system for remotely controlling a social robot using a smartphone’s
motion sensors. Users can select between a first-person perspective from the robot’s
internal camera or a third-person perspective showing the robot’s whole body. Users
can also record their movements for later playback. This system has applications as
an embodied remote communication platform and for crowdsourcing demonstrations of
user-crafted robot movements.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {181},
numpages = {4},
keywords = {crowdsourcing, human-robot interaction, telepresence, motion control},
location = {Yokohama, Japan},
series = {CHI EA '21}
}






@article{yamada2006circulartime,
author = {Yoko Yamada and Yoshinobu Kato},
title ={Images of Circular Time and Spiral Repetition: The Generative Life Cycle Model},
journal = {Culture \& Psychology},
volume = {12},
number = {2},
pages = {143-160},
year = {2006},
doi = {10.1177/1354067X06064575},

URL = { 
        https://doi.org/10.1177/1354067X06064575
    
},
eprint = { 
        https://doi.org/10.1177/1354067X06064575
    
}
,
    abstract = { The purpose of this article is to propose the Generative Life Cycle Model (GLCM) in two versions. The ecological version of the GLCM focuses on the cyclical images of successive generations beyond an individual’s life and death within ecological contexts. The spiral version of the GLCM is represented by the repeated spirals with variants, and is characterized by the multiple time concepts and the generative processes of life and death. The linear time concept, represented as a horizontal arrow, reflects an individual’s life characterized by a single stream of time from birth to death. Certainly, each individual is unique and can never be reproduced. Nevertheless, people can image and narrate stories in which their lives are regarded as being reproducible. These life stories based on repetition, reproduction, recovery and renewal have significant meanings for generative connections from past generations to future generations, and for the imaginations of continuities of multiple human lives within different historical eras }
}



@inproceedings{rakita2020latency,
author = {Rakita, Daniel and Mutlu, Bilge and Gleicher, Michael},
title = {Effects of Onset Latency and Robot Speed Delays on Mimicry-Control Teleoperation},
year = {2020},
isbn = {9781450367462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319502.3374838},
doi = {10.1145/3319502.3374838},
abstract = {In this paper, we study the effects of delays in a mimicry-control robot teleoperation interface which involves a user moving their arms to directly show the robot how to move and the robot follows in real time. Unlike prior work considering delays in other teleoperation systems, we consider delays due to robot slowness in addition to latency in the onset of movement commands. We present a human-subjects study that shows how different amounts and types of delays have different effects on task performance. We compare the movements under different delays to reveal the strategies that operators use to adapt to delay conditions and to explain performance differences. Our results show that users can quickly develop strategies to adapt to slowness delays but not onset latency delays. We discuss the implications of our results for the future development of methods designed to mitigate the effects of delays.},
booktitle = {Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {519-527},
numpages = {9},
keywords = {motion retargeting, teleoperation, latency analysis},
location = {Cambridge, United Kingdom},
series = {HRI '20},
doi={10.1007/978-3-662-44651-5_9},
url={https://doi.org/10.1007/978-3-662-44651-5_9}
}

@inproceedings{tanaka2014comparing,
author="Tanaka, Kazuaki
and Nakanishi, Hideyuki
and Ishiguro, Hiroshi",
title="Comparing Video, Avatar, and Robot Mediated Communication: Pros and Cons of Embodiment",
booktitle="Collaboration Technologies and Social Computing",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="96--110",
abstract="In recent years, studies have begun on robot conferencing as a new telecommunication medium. In robot conferencing, people talk with a remote conversation partner through teleoperated robots which present the bodily motions of the partner with a physical embodiment. However, the effects of physical embodiment on distant communication had not yet been demonstrated. In this study, to find the effects, we conducted an experiment in which subjects talked with a partner through robots and various existing communication media (e.g. voice, avatar and video chats). As a result, we found that the physical embodiment enhanced social telepresence, i.e., the sense of resembling face-to-face interaction. Furthermore, the result implied that physical embodiment built the sense of tension as in the case of a first face-to-face meeting.",
isbn="978-3-662-44651-5"
}


@inproceedings{ainasoja2019smartphone,
  title={Smartphone Teleoperation for Self-balancing Telepresence Robots.},
  author={Ainasoja, Antti E. and Pertuz, Said and K{\"a}m{\"a}r{\"a}inen, Joni-Kristian},
  booktitle={VISIGRAPP (4: VISAPP)},
  pages={561--568},
  year={2019},
  doi={10.5220/0007406405610568}
}

@article{jonggil2018sprint,
  title={SPRinT: A Mixed Approach to a Hand-Held Robot Interface for Telepresence},
  author={Jonggil, Ahn and Kim, Gerard Jounghyun},
  journal={International Journal of Social Robotics},
  volume={10},
  number={4},
  pages={537--552},
  year={2018},
  publisher={Springer Nature BV},
  doi={10.1007/s12369-017-0463-2}
}

@misc{murphy2020covid,
      title={Applications of Robots for COVID-19 Response}, 
      author={Robin R. Murphy and Vignesh Babu Manjunath Gandudi and Justin Adams},
      year={2020},
      eprint={2008.06976},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@inproceedings{lau2020remotehri,
  title={A JavaScript Framework for Crowdsourced Human-Robot Interaction Experiments: RemoteHRI},
  author={Lau, Finley and Gopinath, Deepak and Argall, Brenna D.},
  booktitle={Association for the Advancement of Artificial Intelligence},
  year={2020}
}


@online{deviceorientation,
author={W3C},
title = {DeviceOrientation Event Specification},
url = {https://www.w3.org/TR/orientation-event/},
year={2019}
}

@online{deviceorientation,
author={W3C},
title = {DeviceOrientation Event Specification},
url = {https://www.w3.org/TR/orientation-event/},
year={2019}
}


@article{feil2020covid,
author = {Feil-Seifer, David and Haring, Kerstin S. and Rossi, Silvia and Wagner, Alan R. and Williams, Tom},
title = {Where to Next? The Impact of COVID-19 on Human-Robot Interaction Research},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
url = {https://doi.org/10.1145/3405450},
doi = {10.1145/3405450},
abstract = {The COVID-19 pandemic will have a profound and long-lasting impact on the entire scientific endeavor. Scientists already are adapting research programs to adapt to changes in what is prioritized—and what is possible; educators are changing the way that the next generation of researchers are trained, and flagship conferences in many fields are being cancelled, postponed, and fundamentally transformed.These broad-reaching changes are particularly impactful to human-oriented domains such as human-robot interaction (HRI). Because in-person human-subject experiments can take a year or more to conduct, the research we will see published in the field in the immediate future may appear to be “business as usual,” with accounts of laboratory studies with large numbers of in-person participants. The research currently being performed, however, is of course a different story entirely. Studies that were under way when the current crisis began will be truncated, resulting either in work that cannot be published or in work whose true impact is difficult to accurately assess. Yet HRI research performed in the coming years will be changed in fundamentally different ways; the inability to perform—or expect future performance of—in-person human subjects research, especially research involving tactile or multiparty interaction, will change both the dominant methodological techniques employed by HRI researchers and the very research questions that the field chooses to—and is able to—address.These challenges demand that HRI researchers identify precisely how the field can maintain research quality and impact while the ability to conduct human-subject studies is severely impaired for an undetermined amount of time. A natural inclination may be simply to wait the crisis out in the hope of a speedy return to normalcy; however, in this article, we argue that the community can also take this opportunity to reevaluate and refocus how research in this field is conducted and how students are mentored in ways that will yield benefits for years to come after the current crisis has ended.},
journal = {J. Hum.-Robot Interact.},
month = jun,
articleno = {1},
numpages = {7},
keywords = {COVID-19 impact, Human-robot interaction, research}
}


@article{grollman2018content,
  Abstract = {Robot personalities are today painstakingly hand-crafted by content creators who specify motions, facial animations, and scripts in order to convey a unifying sense of character via interactions. While user preferences can be incorporated by modifying these behaviors to some degree, such as by inserting a user's name or preference into a script, by and large new content must be continually created in order to maintain the robot's `freshness.'This content treadmill represents a long tail of work that diverts resources away from improving the underlying capabilities of the robot. Here we outline a novel method for defining robot personality that allows each robot to have and express a unique personality through varying behaviors that reduces the need for content creation.},
  Author = {Grollman, Daniel H. },
  Da = {2018/04/01},
  Date-Added = {2021-02-14 04:40:40 +0000},
  Date-Modified = {2021-02-14 04:40:40 +0000},
  Doi = {10.1007/s12369-017-0451-6},
  Id = {Grollman2018},
  Isbn = {1875-4805},
  Journal = {International Journal of Social Robotics},
  Number = {2},
  Pages = {225--234},
  Title = {Avoiding the Content Treadmill for Robot Personalities},
  Ty = {JOUR},
  Url = {https://doi.org/10.1007/s12369-017-0451-6},
  Volume = {10},
  Year = {2018},
  Bdsk-Url-1 = {https://doi.org/10.1007/s12369-017-0451-6}
}

@INPROCEEDINGS{taheri2015webrtcbench,
author={{Taheri}, Sajjad and {Beni}, Laleh Aghababaie Beni and {Veidenbaum}, Alexander V. and {Nicolau}, Alexandru and {Cammarota}, Rosario and {Qiu}, Jianlin and {Lu}, Qiang and {Haghighat}, Mohammad R.},
booktitle={2015 13th IEEE Symposium on Embedded Systems For Real-time Multimedia (ESTIMedia)}, title={WebRTCbench: a benchmark for performance assessment of webRTC implementations},
year={2015},
volume={},
number={},
pages={1-7},
doi={10.1109/ESTIMedia.2015.7351769}}

@InProceedings{raaen2015latency,
author="Raaen, Kjetil
and Kjellmo, Ivar",
title="Measuring Latency in Virtual Reality Systems",
booktitle="Entertainment Computing - ICEC 2015",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="457--462",
abstract="Virtual Reality(VR) systems have the potential to revolutionise how we interact with computers. However motion sickness and discomfort are currently severely impeding the adoption. Traditionally the focus of optimising VR systems have been on frame-rate. Delay and frame-rate are however not equivalent. Latency may occur in several steps in image processing, and a frame-rate measure only picks up some of them. We have made an experimental setup to physically measure the actual delay from the user moves the head until the screen of the VR device is updated. Our results show that while dedicated VR-equipment had very low delay, smartphones are in general not ready for VR-applications.",
isbn="978-3-319-24589-8"
}


@inproceedings{tsoi2021vector,
author = {Tsoi, Nathan and Connolly, Joe and Ad\'{e}n\'{\i}ran, Emmanuel and Hansen, Amanda and Pineda, Kaitlynn Taylor and Adamson, Timothy and Thompson, Sydney and Ramnauth, Rebecca and V\'{a}zquez, Marynel and Scassellati, Brian},
title = {Challenges Deploying Robots During a Pandemic: An Effort to Fight Social Isolation Among Children},
year = {2021},
isbn = {9781450382892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434073.3444665},
doi = {10.1145/3434073.3444665},
abstract = {The practice of social distancing during the COVID-19 pandemic resulted in billions of people quarantined in their homes. In response, we designed and deployed VectorConnect, a robot teleoperation system intended to help combat the effects of social distancing in children during the pandemic. VectorConnect uses the off-the-shelf Vector robot to allow its users to engage in physical play while being geographically separated. We distributed the system to hundreds of users in a matter of weeks. This paper details the development and deployment of the system, our accomplishments, and the obstacles encountered throughout this process. Also, it provides recommendations to best facilitate similar deployments in the future. We hope that this case study about Human-Robot Interaction practice serves as an inspiration to innovate in times of global crises.},
booktitle = {Proceedings of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {234–242},
numpages = {9},
keywords = {activity-centered design, collaborative and social computing devices},
location = {Boulder, CO, USA},
series = {HRI '21}
}

@INPROCEEDINGS{adams2015decoupling,
  author={Adams, Andra and Mahmoud, Marwa and Baltrušaitis, Tadas and Robinson, Peter},
  booktitle={2015 International Conference on Affective Computing and Intelligent Interaction (ACII)}, 
  title={Decoupling facial expressions and head motions in complex emotions}, 
  year={2015},
  volume={},
  number={},
  pages={274-280},
  doi={10.1109/ACII.2015.7344583}}

  @article{yoon2020speech,
author = {Yoon, Youngwoo and Cha, Bok and Lee, Joo-Haeng and Jang, Minsu and Lee, Jaeyeon and Kim, Jaehong and Lee, Geehyuk},
title = {Speech Gesture Generation from the Trimodal Context of Text, Audio, and Speaker Identity},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3414685.3417838},
doi = {10.1145/3414685.3417838},
abstract = {For human-like agents, including virtual avatars and social robots, making proper gestures while speaking is crucial in human-agent interaction. Co-speech gestures enhance interaction experiences and make the agents look alive. However, it is difficult to generate human-like gestures due to the lack of understanding of how people gesture. Data-driven approaches attempt to learn gesticulation skills from human demonstrations, but the ambiguous and individual nature of gestures hinders learning. In this paper, we present an automatic gesture generation model that uses the multimodal context of speech text, audio, and speaker identity to reliably generate gestures. By incorporating a multimodal context and an adversarial training scheme, the proposed model outputs gestures that are human-like and that match with speech content and rhythm. We also introduce a new quantitative evaluation metric for gesture generation models. Experiments with the introduced metric and subjective human evaluation showed that the proposed gesture generation model is better than existing end-to-end generation models. We further confirm that our model is able to work with synthesized audio in a scenario where contexts are constrained, and show that different gesture styles can be generated for the same speech by specifying different speaker identities in the style embedding space that is learned from videos of various speakers. All the code and data is available at https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.},
journal = {ACM Transactions on Graphics},
month = nov,
articleno = {222},
keywords = {co-speech gesture, neural generative model, evaluation of a generative model, nonverbal behavior, multimodality}
}

@article{kucherenko2021large,
  title={A Large, Crowdsourced Evaluation of Gesture Generation Systems on Common Data: The GENEA Challenge 2020},
  author={Kucherenko, Taras and Jonell, Patrik and Yoon, Youngwoo and Wolfert, Pieter and Henter, Gustav Eje},
  journal={International Conference on Intelligent User Interfaces},
  year={2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url={https://doi.org/10.1145/3397481.3450692},
  doi={10.1145/3397481.3450692},
}

@inproceedings{suguitan2020moveae,
author = {Suguitan, Michael and Gomez, Randy and Hoffman, Guy},
title = {MoveAE: Modifying Affective Robot Movements Using Classifying Variational Autoencoders},
year = {2020},
isbn = {9781450367462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319502.3374807},
doi = {10.1145/3319502.3374807},
abstract = {We propose a method for modifying affective robot movements using neural networks. Social robots use gestures and other movements to express their internal states. However, a robot's interactive capabilities are hindered by the predominant use of a limited set of preprogrammed or hand-animated behaviors, which can be repetitive and predictable, making sustained human-robot interactions difficult to maintain. To address this, we developed a method for modifying existing emotive robot movements by using neural networks. We use hand-crafted movement samples and a classifying variational autoencoder trained on these samples. Our method then allows for adjustment of affective movement features by using simple arithmetic in the network's latent embedding space. We present the implementation and evaluation of this approach and show that editing in the latent space can modify the emotive quality of the movements while preserving recognizability and legibility in many cases. This supports neural networks as viable tools for creating and modifying expressive robot behaviors.},
booktitle = {Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {481–489},
numpages = {9},
keywords = {affective generation, deep learning, neural networks, social robots},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}

@INPROCEEDINGS{ioffe2015batchnorm, title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, author = {Sergey Ioffe and Christian Szegedy}, booktitle = {Proceedings of the 32nd International Conference on Machine Learning}, pages = {448--456}, year = {2015}, editor = {Francis Bach and David Blei}, volume = {37}, series = {Proceedings of Machine Learning Research}, address = {Lille, France}, month = {07--09 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v37/ioffe15.pdf}, url = { http://proceedings.mlr.press/v37/ioffe15.html }, abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.} }

@inproceedings{komiya2017jackin,
author = {Komiyama, Ryohei and Miyaki, Takashi and Rekimoto, Jun},
title = {JackIn Space: Designing a Seamless Transition between First and Third Person View for Effective Telepresence Collaborations},
year = {2017},
isbn = {9781450348355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3041164.3041183},
doi = {10.1145/3041164.3041183},
abstract = {Traditional telepresence systems only supported first person view and users had difficulty in recognizing the surrounding situation of their remote workspace. JackIn Space is a telepresence system that solves this problem by seamlessly integrating first person view with third person view. With a head-mounted first person camera and multiple depth sensors installed in the environment, the surrogate user's first person view smoothly changes to the out-of-body third person view, and the user connected to the surrogate user can virtually look around the environment. The user can also dive into other surrogate users to gain different perspectives. Our evaluation supports that the concept and the function of JackIn Space was quite well accepted and our prototype system provides a more natural viewpoint selection. Overall, JackIn Space supports better remote collaborations.},
booktitle = {Proceedings of the 8th Augmented Human International Conference},
articleno = {14},
keywords = {augmented reality, first person view, kinect, telepresence, 3D modeling, remote collaboration},
location = {Silicon Valley, California, USA},
series = {AH '17}
}


@ARTICLE{sugimoto2005time,
  author={M. {Sugimoto} and G. {Kagotani} and H. {Nii} and N. {Shiroma} and F. {Matsuno} and M. {Inami}},
  journal={IEEE Computer Graphics and Applications}, 
  title={Time Follower's Vision: a teleoperation interface with past images}, 
  year={2005},
  volume={25},
  number={1},
  pages={54-63},
  doi={10.1109/MCG.2005.23}}

  @ARTICLE{cao2019openpose,
  author={Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields}, 
  year={2019},
  volume={43},
  number={1},
  pages={172-186},
  doi={10.1109/TPAMI.2019.2929257}}

@InProceedings{pavllo2019videopose,
author = {Pavllo, Dario and Feichtenhofer, Christoph and Grangier, David and Auli, Michael},
title = {3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@InProceedings{siarohin2021motion,
    author    = {Siarohin, Aliaksandr and Woodford, Oliver J. and Ren, Jian and Chai, Menglei and Tulyakov, Sergey},
    title     = {Motion Representations for Articulated Animation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {13653-13662}
}

@misc{hinz2021charactergan,
      title={CharacterGAN: Few-Shot Keypoint Character Animation and Reposing}, 
      author={Tobias Hinz and Matthew Fisher and Oliver Wang and Eli Shechtman and Stefan Wermter},
      year={2021},
      eprint={2102.03141},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ramesh2021zeroshot,
      title={Zero-Shot Text-to-Image Generation}, 
      author={Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
      year={2021},
      eprint={2102.12092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{bai2018imagecaptioning,
title = {A survey on automatic image caption generation},
journal = {Neurocomputing},
volume = {311},
pages = {291-304},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.05.080},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218306659},
author = {Shuang Bai and Shan An},
keywords = {Image captioning, Sentence template, Deep neural networks, Multimodal embedding, Encoder–decoder framework, Attention mechanism},
abstract = {Image captioning means automatically generating a caption for an image. As a recently emerged research area, it is attracting more and more attention. To achieve the goal of image captioning, semantic information of images needs to be captured and expressed in natural languages. Connecting both research communities of computer vision and natural language processing, image captioning is a quite challenging task. Various approaches have been proposed to solve this problem. In this paper, we present a survey on advances in image captioning research. Based on the technique adopted, we classify image captioning approaches into different categories. Representative methods in each category are summarized, and their strengths and limitations are talked about. In this paper, we first discuss methods used in early work which are mainly retrieval and template based. Then, we focus our main attention on neural network based methods, which give state of the art results. Neural network based methods are further divided into subcategories based on the specific framework they use. Each subcategory of neural network based methods are discussed in detail. After that, state of the art methods are compared on benchmark datasets. Following that, discussions on future research directions are presented.}
}

@ARTICLE{baltrusaitis2019multimodal,
  author={Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Multimodal Machine Learning: A Survey and Taxonomy}, 
  year={2019},
  volume={41},
  number={2},
  pages={423-443},
  doi={10.1109/TPAMI.2018.2798607}}
  
@InProceedings{nguyen2021manifold,
    author    = {Nguyen, Andre T. and Richards, Luke E. and Kebe, Gaoussou Youssouf and Raff, Edward and Darvish, Kasra and Ferraro, Frank and Matuszek, Cynthia},
    title     = {Practical Cross-Modal Manifold Alignment for Robotic Grounded Language Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2021},
    pages     = {1613-1622}
}

@inproceedings{dossantos2014sentiment,
    title = "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts",
    author = "dos Santos, C{\'\i}cero  and
      Gatti, Ma{\'\i}ra",
    booktitle = "Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Dublin City University and Association for Computational Linguistics",
    url = "https://aclanthology.org/C14-1008",
    pages = "69--78",
}

@article{jain2019emotion,
title = {Extended deep neural network for facial emotion recognition},
journal = {Pattern Recognition Letters},
volume = {120},
pages = {69-74},
year = {2019},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S016786551930008X},
author = {Deepak Kumar Jain and Pourya Shamsolmoali and Paramjit Sehdev},
keywords = {Facial emotion recognition, Deep neural network, Fully convolution network},
abstract = {Humans use facial expressions to show their emotional states. However, facial expression recognition has remained a challenging and interesting problem in computer vision. In this paper we present our approach which is the extension of our previous work for facial emotion recognition [1]. The aim of this work is to classify each image into one of six facial emotion classes. The proposed model is based on single Deep Convolutional Neural Networks (DNNs), which contain convolution layers and deep residual blocks. In the proposed model, firstly the image label to all faces has been set for the training. Secondly, the images go through proposed DNN model. This model trained on two datasets Extended Cohn–Kanade (CK+) and Japanese Female Facial Expression (JAFFE) Dataset. The overall results show that, the proposed DNN model can outperform the recent state-of-the-art approaches for emotion recognition. Even the proposed model has accuracy improvement in comparison with our previous model.}
}

@InProceedings{he2016resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@INPROCEEDINGS{cohnkanade,
  author={Kanade, T. and Cohn, J.F. and Yingli Tian},
  booktitle={Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)}, 
  title={Comprehensive database for facial expression analysis}, 
  year={2000},
  volume={},
  number={},
  pages={46-53},
  doi={10.1109/AFGR.2000.840611}}
  
@misc{mcinnes2020umap,
      title={UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}, 
      author={Leland McInnes and John Healy and James Melville},
      year={2020},
      eprint={1802.03426},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{kleinsmith2006cultural,
    author = {Kleinsmith, Andrea and De Silva, P. Ravindra and Bianchi-Berthouze, Nadia},
    title = "{Cross-cultural differences in recognizing affect from body posture}",
    journal = {Interacting with Computers},
    volume = {18},
    number = {6},
    pages = {1371-1389},
    year = {2006},
    month = {06},
    abstract = "{Conveyance and recognition of human emotion and affective expression is influenced by many factors, including culture. Within the user modeling field, it has become increasingly necessary to understand the role affect can play in personalizing interactive interfaces using embodied animated agents. However, little research within the computer science field aims at understanding cultural differences within this vein. Therefore, we conducted a study to evaluate if differences exist in the way various cultures perceive emotion from body posture. We used static posture images of affectively expressive avatars to conduct recognition experiments with subjects from three cultures. After analyzing the subjects' judgments using multivariate analysis, we grounded the identified differences into a set of low-level posture features. We then used Mixture Discriminant Analysis (MDA) and an unsupervised expectation maximization (EM) model to build separate cultural models for affective posture recognition. Our results could prove useful to aide designers in creating more effective affective avatars.}",
    issn = {0953-5438},
    doi = {10.1016/j.intcom.2006.04.003},
    url = {https://doi.org/10.1016/j.intcom.2006.04.003},
    eprint = {https://academic.oup.com/iwc/article-pdf/18/6/1371/1996265/iwc18-1371.pdf},
}



@incollection{burton2016laban,
  title={Laban movement analysis and affective movement generation for robots and other near-living creatures},
  author={Burton, Sarah Jane and Samadani, Ali-Akbar and Gorbet, Rob and Kuli{\'c}, Dana},
  booktitle={Dance notations and robot motion},
  pages={25--48},
  year={2016},
  publisher={Springer}
}

@misc{cotter2018deep,
      title={Deep Learning in the Wavelet Domain}, 
      author={Fergal Cotter and Nick Kingsbury},
      year={2018},
      eprint={1811.06115},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhumekenov2019fourier,
      title={Fourier Neural Networks: A Comparative Study}, 
      author={Abylay Zhumekenov and Malika Uteuliyeva and Olzhas Kabdolov and Rustem Takhanov and Zhenisbek Assylbekov and Alejandro J. Castro},
      year={2019},
      eprint={1902.03011},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{saxena2020emotion,
  title={Emotion recognition and detection methods: A comprehensive survey},
  author={Saxena, Anvita and Khanna, Ashish and Gupta, Deepak},
  journal={Journal of Artificial Intelligence and Systems},
  volume={2},
  number={1},
  pages={53--79},
  year={2020},
  publisher={Institute of Electronics and Computer}
}

@InProceedings{hussain2019chatbot,
author="Hussain, Shafquat
and Ameri Sianaki, Omid
and Ababneh, Nedal",
title="A Survey on Conversational Agents/Chatbots Classification and Design Techniques",
booktitle="Web, Artificial Intelligence and Network Applications",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="946--956",
abstract="A chatbot can be defined as a computer program, designed to interact with users using natural language or text in a way that the user thinks he is having dialogue with a human. Most of the chatbots utilise the algorithms of artificial intelligence (AI) in order to generate required response. Earlier chatbots merely created an illusion of intelligence by employing much simpler pattern matching and string processing design techniques for their interaction with users using rule-based and generative-based models. However, with the emergence of new technologies more intelligent systems have emerged using complex knowledge-based models. This paper aims to discuss chatbots classification, their design techniques used in earlier and modern chatbots and how the two main categories of chatbots handle conversation context.",
isbn="978-3-030-15035-8"
}


@phdthesis{marmpena2021emotional,
  title={Emotional body language synthesis for humanoid robots},
  author={Marmpena, Mina},
  year={2021},
  school={University of Plymouth}
}


@misc{nojavanasghari2018gan,
      title={Interactive Generative Adversarial Networks for Facial Expression Generation in Dyadic Interactions}, 
      author={Behnaz Nojavanasghari and Yuchi Huang and Saad Khan},
      year={2018},
      eprint={1801.09092},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{lubis2018emotion, title={Eliciting Positive Emotion through Affect-Sensitive Dialogue Response Generation: A Neural Network Approach}, volume={32}, url={https://ojs.aaai.org/index.php/AAAI/article/view/11955}, abstractNote={ &lt;p&gt; An emotionally-competent computer agent could be a valuable assistive technology in performing various affective tasks. For example caring for the elderly, low-cost ubiquitous chat therapy, and providing emotional support in general, by promoting a more positive emotional state through dialogue system interaction. However, despite the increase of interest in this task, existing works face a number of shortcomings: system scalability, restrictive modeling, and weak emphasis on maximizing user emotional experience. In this paper, we build a fully data driven chat-oriented dialogue system that can dynamically mimic affective human interactions by utilizing a neural network architecture. In particular, we propose a sequence-to-sequence response generator that considers the emotional context of the dialogue. An emotion encoder is trained jointly with the entire network to encode and maintain the emotional context throughout the dialogue. The encoded emotion information is then incorporated in the response generation process. We train the network with a dialogue corpus that contains positive-emotion eliciting responses, collected through crowd-sourcing. Objective evaluation shows that incorporation of emotion into the training process helps reduce the perplexity of the generated responses, even when a small dataset is used. Subsequent subjective evaluation shows that the proposed method produces responses that are more natural and likely to elicit a more positive emotion. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Lubis, Nurul and Sakti, Sakriani and Yoshino, Koichiro and Nakamura, Satoshi}, year={2018}, month={Apr.} }

@inproceedings{pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@INPROCEEDINGS{leite2009novelty,
  author={Leite, Iolanda and Martinho, Carlos and Pereira, Andre and Paiva, Ana},
  booktitle={RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication}, 
  title={As Time goes by: Long-term evaluation of social presence in robotic companions}, 
  year={2009},
  volume={},
  number={},
  pages={669-674},
  doi={10.1109/ROMAN.2009.5326256}}

@article{robertson2020uncanny,
title = "No Place for Robots: Reassessing the Bukimi no Tani",
journal = "The Asia-Pacific Journal | Japan Focus",
volume = "18",
issue = "23",
number = "4",
year = "2020",
author = "Robertson, Jennifer",
}


@article{plato370BCphaedrus,
  title={Phaedrus},
  author={Plato},
  year={370 BC}
}

@article{williams2021misfit,
  title={I, Misfit: Empty Fortresses, Social Robots, and Peculiar Relations in Autism Research},
  author={Williams, Rua M},
  journal={Techn{\'e}: Research in Philosophy and Technology},
  year={2021}
}

@book{turkle2017alone,
  title={Alone Together: Why We Expect More from Technology and Less from Each Other},
  author={Turkle, Sherry},
  year={2017},
  publisher={Hachette UK}
}

@article{gaver2012annotated,
  title={Annotated portfolios},
  author={Gaver, Bill and Bowers, John},
  journal={Interactions},
  volume={19},
  number={4},
  pages={40--49},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@article{levecque2017work,
  title={Work organization and mental health problems in PhD students},
  author={Levecque, Katia and Anseel, Frederik and De Beuckelaer, Alain and Van der Heyden, Johan and Gisle, Lydia},
  journal={Research Policy},
  volume={46},
  number={4},
  pages={868--879},
  year={2017},
  publisher={Elsevier}
}

@article{loissel2019mental,
  title={Mental Health in Academia: A question of support},
  author={Loissel, Elsa},
  journal={Elife},
  volume={8},
  pages={e52881},
  year={2019},
  publisher={eLife Sciences Publications Limited}
}

@article{moulin2020mental,
  title={Mental health in academia: The role of workplace relationships},
  author={Moulin, Thiago C},
  journal={Frontiers in Psychology},
  volume={11},
  year={2020},
  publisher={Frontiers Media SA}
}

@INPROCEEDINGS{lacey2019cuteness,
  author={Lacey, Cherie and Caudwell, Catherine},
  booktitle={2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Cuteness as a ‘Dark Pattern’ in Home Robots}, 
  year={2019},
  volume={},
  number={},
  pages={374-381},
  doi={10.1109/HRI.2019.8673274}}

  @inproceedings{bartneck2018racism,
author = {Bartneck, Christoph and Yogeeswaran, Kumar and Ser, Qi Min and Woodward, Graeme and Sparrow, Robert and Wang, Siheng and Eyssel, Friederike},
title = {Robots And Racism},
year = {2018},
isbn = {9781450349536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3171221.3171260},
doi = {10.1145/3171221.3171260},
abstract = {Most robots currently being sold or developed are either stylized with white material or have a metallic appearance. In this research we used the shooter bias paradigm and several questionnaires to investigate if people automatically identify robots as being racialized, such that we might say that some robots are 'White' while others are 'Asian', or 'Black'. To do so, we conducted an extended replication of the classic social psychological shooter bias paradigm using robot stimuli to explore whether effects known from human-human intergroup experiments would generalize to robots that were racialized as Black and White. Reaction-time based measures revealed that participants demonstrated 'shooter-bias' toward both Black people and robot racialized as Black. Participants were also willing to attribute a race to the robots depending on their racialization and demonstrated a high degree of inter-subject agreement when it came to these attributions.},
booktitle = {Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {196–204},
numpages = {9},
keywords = {robot, implicit, shooter bias, explicit, racism, prejudice},
location = {Chicago, IL, USA},
series = {HRI '18}
}

@article{horstmann2018turnoff,
    doi = {10.1371/journal.pone.0201581},
    author = {Horstmann, Aike C. AND Bock, Nikolai AND Linhuber, Eva AND Szczuka, Jessica M. AND Straßmann, Carolin AND Krämer, Nicole C.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Do a robot’s social skills and its objection discourage interactants from switching the robot off?},
    year = {2018},
    month = {07},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0201581},
    pages = {1-25},
    abstract = {Building on the notion that people respond to media as if they were real, switching off a robot which exhibits lifelike behavior implies an interesting situation. In an experimental lab study with a 2x2 between-subjects-design (N = 85), people were given the choice to switch off a robot with which they had just interacted. The style of the interaction was either social (mimicking human behavior) or functional (displaying machinelike behavior). Additionally, the robot either voiced an objection against being switched off or it remained silent. Results show that participants rather let the robot stay switched on when the robot objected. After the functional interaction, people evaluated the robot as less likeable, which in turn led to a reduced stress experience after the switching off situation. Furthermore, individuals hesitated longest when they had experienced a functional interaction in combination with an objecting robot. This unexpected result might be due to the fact that the impression people had formed based on the task-focused behavior of the robot conflicted with the emotional nature of the objection.},
    number = {7},

}

@book{richie2007tractate,
  title={A tractate on Japanese aesthetics},
  author={Richie, Donald},
  year={2007},
  publisher={Stone Bridge Press}
}

@inproceedings{blythe2017fiction,
author = {Blythe, Mark},
title = {Research Fiction: Storytelling, Plot and Design},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3026023},
doi = {10.1145/3025453.3026023},
abstract = {What kind of stories and plots do researchers of Human Computer Interaction draw on when they make fictions? This paper applies the "basic plots" identified in the study of literature to scenarios, speculative design and design fiction. Traditional HCI scenarios employ the plot of "Overcoming the Monster" where the monster is some problem to be solved. Much of the commentary on critical, speculative or adversarial design also draws on this plot as it attempts to overcome monsters like public apathy or a lack of debate. Design Fiction more frequently takes the form of a "Voyage and Return" or a "Quest". The paper argues that a better understanding of plot and storytelling could contribute to more reflective research fiction.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5400–5411},
numpages = {12},
keywords = {solutionism, design fiction, critical design, scenarios, speculative design, personas, adversarial design},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@incollection{grvzinic2000exposure,
  title={Exposure time, the aura, and telerobotics},
  author={Gr{\v{z}}ini{\'c}, Marina},
  booktitle={The Robot in the Garden: Telerobotics and Telepistemology in the Age of the Internet},
  pages={214--224},
  year={2000}
}








@article{majumdar2020screen,
author = {Piya Majumdar and Ankita Biswas and Subhashis Sahu},
title = {{COVID-19} pandemic and lockdown: cause of sleep disruption, depression, somatic pain, and increased screen exposure of office workers and students of India},
journal = {Chronobiology International},
volume = {37},
number = {8},
pages = {1191-1200},
year  = {2020},
publisher = {Taylor & Francis},
doi = {10.1080/07420528.2020.1786107},
    note ={PMID: 32660352},

URL = { 
        https://doi.org/10.1080/07420528.2020.1786107
    
},
eprint = { 
        https://doi.org/10.1080/07420528.2020.1786107
    
}

}


@article{dollinger1987mirror,
  abstract = {It was predicted that attention focused on the self would interfere with performance on a task that requires recognizing the psychological implications of word associations. In particular, it was expected that the combination of a high level of test anxiety or self-consciousness and experimental testing conditions (i.e., instructions about task importance, unveiling of a mirror, and request for permission to videotape) would produce especially poor performance. Results revealed a main effect of testing condition such that subjects in the experimental condition were less vigilant and less successful in their performance than were subjects in the control condition. The results are discussed in terms of Scheibe's (1979) concept of the interpersonal prediction mode of sagacity and the ``mirror''as a tool for avoiding prediction.},
  author = {Dollinger, Stephen J. and Greening, Leilani and Lloyd, Karen},
  da = {1987/03/01},
  date-added = {2021-11-23 15:34:21 -0500},
  date-modified = {2021-11-23 15:34:21 -0500},
  doi = {10.3758/BF03330318},
  id = {Dollinger1987},
  isbn = {0090-5054},
  journal = {Bulletin of the Psychonomic Society},
  number = {3},
  pages = {167--170},
  title = {The ``mirror'' and the ``mask'': Self-focused attention, evaluation anxiety, and the recognition of psychological implications},
  ty = {JOUR},
  url = {https://doi.org/10.3758/BF03330318},
  volume = {25},
  year = {1987},
  Bdsk-Url-1 = {https://doi.org/10.3758/BF03330318}}




@book{eno1996appendices,
  title={A year with swollen appendices: Brian Eno's Diary},
  author={Eno, Brian},
  year={1996},
  publisher={Faber \& Faber}
}

@ARTICLE{shannon1948communication,
  author={Shannon, C. E.},
  journal={The Bell System Technical Journal}, 
  title={A mathematical theory of communication}, 
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  doi={10.1002/j.1538-7305.1948.tb01338.x}}

  @misc{bardzell2019artifact,
  author={Bardzell, Jeffrey},
  title={Knowledge Embodied in Artifacts: A Problem in Design Epistemology},
  journal={Carnegie Mellon University Human-Computer Interaction Institute Seminar},
  year={2019},
  month={Oct}
  }

  @misc{luria2019championing,
      title={Championing Research Through Design in {HRI}}, 
      author={Michal Luria and John Zimmerman and Jodi Forlizzi},
      year={2019},
      eprint={1908.07572},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@article{komatsu2012expectations,
  title={How does the difference between users’ expectations and perceptions about a robotic agent affect their behavior?},
  author={Komatsu, Takanori and Kurosawa, Rie and Yamada, Seiji},
  journal={International Journal of Social Robotics},
  volume={4},
  number={2},
  pages={109--116},
  year={2012},
  publisher={Springer}
}

@incollection{ogawa2013android,
  title={Android robots as telepresence media},
  author={Ogawa, Kohei and Nishio, Shuichi and Minato, Takashi and Ishiguro, Hiroshi},
  booktitle={Biomedical Engineering and Cognitive Neuroscience for Healthcare: Interdisciplinary Applications},
  pages={54--63},
  year={2013},
  publisher={IGI Global}
}

@article{nishio2007geminoid,
  title={Geminoid: Teleoperated android of an existing person},
  author={Nishio, Shuichi and Ishiguro, Hiroshi and Hagita, Norihiro},
  journal={Humanoid robots: New developments},
  volume={14},
  pages={343--352},
  year={2007},
  publisher={Vienna, Austria: I-Tech Education and Publishing}
}

@INPROCEEDINGS{wiltgen2010roomba,
  author={Wiltgen, Bryan and Beer, Jenay M. and McGreggor, Keith and Jiang, Karl and Thomaz, Andrea},
  booktitle={19th International Symposium in Robot and Human Interactive Communication}, 
  title={The interplay of context and emotion for non-anthropomorphic robots}, 
  year={2010},
  volume={},
  number={},
  pages={658-663},
  doi={10.1109/ROMAN.2010.5598669}}

  @phdthesis{harris2011stem,
  title={Exploring the Affect of Emotive Motion in Social Human Robot Interaction},
  author={Harris, John},
  year={2011},
  school={University of Calgary}
}

@book{damasio2006descartes,
  title={Descartes' Error},
  author={Damasio, Antonio R},
  year={2006},
  publisher={Random House}
}

@article{kato2019hikikomori,
author = {Kato, Takahiro A. and Kanba, Shigenobu and Teo, Alan R.},
title = {Hikikomori : Multidimensional understanding, assessment, and future international perspectives},
journal = {Psychiatry and Clinical Neurosciences},
volume = {73},
number = {8},
pages = {427-440},
keywords = {amae, hikikomori, modern-type depression, social anxiety, social isolation},
doi = {https://doi.org/10.1111/pcn.12895},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/pcn.12895},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/pcn.12895},
abstract = {Hikikomori, a severe form of social withdrawal, has long been observed in Japan mainly among youth and adolescents since around the 1970s, and has been especially highlighted since the late 1990s. Moreover, hikikomori-like cases have recently been reported in many other countries. Hikikomori negatively influences not only the individual's mental health and social participation, but also wider education and workforce stability, and as such is a novel urgent global issue. In this review, we introduce the history, definition, diagnostic evaluation, and interventions for hikikomori and also the international prevalence of hikikomori outside Japan. We propose a hypothesis regarding the globalization of hikikomori based on domestic and international perspectives. In addition, we introduce our latest assessment system for hikikomori (including the latest version of the ‘proposed diagnostic criteria of hikikomori for the future DSM/ICD diagnostic systems’) and propose therapeutic strategies, including family approaches and individualized therapies. Finally, we present future challenges that may lead to solutions for an internationalized hikikomori.},
year = {2019}
}

@ARTICLE{tateno2019addiction,
  
AUTHOR={Tateno, Masaru and Teo, Alan R. and Ukai, Wataru and Kanazawa, Junichiro and Katsuki, Ryoko and Kubo, Hiroaki and Kato, Takahiro A.},   
   
TITLE={Internet Addiction, Smartphone Addiction, and Hikikomori Trait in Japanese Young Adult: Social Isolation and Social Network},      
  
JOURNAL={Frontiers in Psychiatry},      
  
VOLUME={10},      

PAGES={455},     
  
YEAR={2019},      
    
URL={https://www.frontiersin.org/article/10.3389/fpsyt.2019.00455},       
  
DOI={10.3389/fpsyt.2019.00455},      
  
ISSN={1664-0640},   
   
ABSTRACT={Background: As the number of internet users increases, problems related to internet overuse are becoming more and more serious. Adolescents and youth may be particularly attracted to and preoccupied with various online activities. In this study, we investigated the relationship of internet addiction, smartphone addiction, and the risk of hikikomori, severe social withdrawal, in Japanese young adult.Methods: The subjects were 478 college/university students in Japan. They were requested to complete the study questionnaire, which consisted of questions about demographics, internet use, the Internet Addiction Test (IAT), the Smartphone Addiction Scale (SAS)–Short Version (SV), the 25-item Hikikomori Questionnaire (HQ-25), etc. We investigated the difference and correlation of the results between two groups based on the purpose of internet use or the total score of each self-rating scale, such as screened positive or negative for the risk of internet addiction, smartphone addiction, or hikikomori.Results: There was a trend that males favored gaming in their internet use while females used the internet mainly for social networking via smartphone, and the mean SAS-SV score was higher in females. Two-group comparisons between gamers and social media users, according to the main purpose of internet use, showed that gamers used the internet longer and had significantly higher mean IAT and HQ-25 scores. Regarding hikikomori trait, the subjects at high risk for hikikomori on HQ-25 had longer internet usage time and higher scores on both IAT and SAS-SV. Correlation analyses revealed that HQ-25 and IAT scores had a relatively strong relationship, although HQ-25 and SAS-SV had a moderately weak one.Discussion: Internet technology has changed our daily lives dramatically and altered the way we communicate as well. As social media applications are becoming more popular, users are connected more tightly to the internet and their time spent with others in the real world continues to decrease. Males often isolate themselves from the social community in order to engage in online gaming while females use the internet as to not be excluded from their communications online. Mental health providers should be aware of the seriousness of internet addictions and hikikomori.}
}

@book{schopenhauer1851parerga,
  title={Parerga and Paralipomena: Short philosophical essays},
  author={Schopenhauer, Arthur},
  volume={1},
  year={1851},
  publisher={Oxford University Press on Demand}
}

@inproceedings{zimmerman2007rtd,
author = {Zimmerman, John and Forlizzi, Jodi and Evenson, Shelley},
title = {Research through Design as a Method for Interaction Design Research in {HCI}},
year = {2007},
isbn = {9781595935939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1240624.1240704},
doi = {10.1145/1240624.1240704},
abstract = {For years the HCI community has struggled to integrate design in research and practice. While design has gained a strong foothold in practice, it has had much less impact on the HCI research community. In this paper we propose a new model for interaction design research within HCI. Following a research through design approach, designers produce novel integrations of HCI research in an attempt to make the right thing: a product that transforms the world from its current state to a preferred state. This model allows interaction designers to make research contributions based on their strength in addressing under-constrained problems. To formalize this model, we provide a set of four lenses for evaluating the research contribution and a set of three examples to illustrate the benefits of this type of research.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {493–502},
numpages = {10},
keywords = {research through design, design method, wicked problems, design, interaction design research, HCI research, interaction design, design theory},
location = {San Jose, California, USA},
series = {CHI '07}
}

@inproceedings{spatola2020mortality,
author = {Spatola, Nicolas},
title = {Would You Turn Off a Robot Because It Confronts You with Your Own Mortality?},
year = {2020},
isbn = {9781450370578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371382.3380736},
doi = {10.1145/3371382.3380736},
abstract = {A future world populated by robots is a projection that has long inspired and still inspires science-fiction stories. As their complexity increase people can help but compare to these artificial entities and why not question their own "human nature". While it is easy to assess the superiority of humans on many dimensions, there is one, central to all humans that is not in favor of our kind: the mortality. In this study, we investigate how individuals react toward a robot that makes this comparison dimension stand out and how it may define attitudes towards robots and pro/antisocial behaviors towards them. Also, we evaluate the role of robot anthropomorphism and the spiritual thoughts activation as mediators of the above-mentioned process. Our study's results demonstrate that facing a robot that confronts people with their own mortality results in less positive attitudes towards robots, and a higher likelihood of acting negatively toward them. Also, this particular robot tends to energize more spiritual thoughts among participants and less human traits attribution. Finally, we show that spirituality may be paradoxically associated with more anthropomorphism and more negative attitude showing the ambiguous multi-component stance of this dimension. These results are discussed in terms of attitudes towards robots, Terror Management Theory, social comparison process and implications for future human-robot interaction (HRI)},
booktitle = {Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {61–68},
numpages = {8},
keywords = {attitude, prosocial behaviour, social agents, human robot comparison, human robot interaction},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}


@inproceedings{sharkey2014paro,
  title={The {Paro} seal robot: demeaning or enabling},
  author={Sharkey, Amanda and Wood, Natalie},
  booktitle={Proceedings of AISB},
  volume={36},
  pages={2014},
  year={2014}
}

@inproceedings{calo2011ethical,
  title={Ethical implications of using the {Paro} robot, with a focus on dementia patient care},
  author={Calo, Christopher James and Hunt-Bull, Nicholas and Lewis, Lundy and Metzler, Ted},
  booktitle={Workshops at the twenty-fifth AAAI conference on artificial intelligence},
  year={2011}
}

@article{hildebrand2021message,
  title={What is the message of the robot medium? Considering media ecology and mobilities in critical robotics research},
  author={Hildebrand, Julia M},
  journal={AI \& Society},
  pages={1--11},
  year={2021},
  publisher={Springer}
}

@article{waytz2014botsourcing,
  title={Botsourcing and outsourcing: Robot, British, Chinese, and German workers are for thinking—not feeling—jobs.},
  author={Waytz, Adam and Norton, Michael I},
  journal={Emotion},
  volume={14},
  number={2},
  pages={434},
  year={2014},
  publisher={American Psychological Association}
}

@phdthesis{mays2021humanizing,
  title={Humanizing robots? The influence of appearance and status on social perceptions of robots},
  author={Mays, Kate Keener},
  year={2021},
  school={Boston University}
} 

@article{drago2015effect,
  title={The effect of technology on face-to-face communication},
  author={Drago, Emily},
  journal={Elon Journal of Undergraduate Research in Communications},
  volume={6},
  number={1},
  year={2015}
}

@article{taipale2018communicating,
  title={Communicating with machines: robots as the next new media},
  author={Taipale, S and Fortunati, L},
  journal={Human-machine communication: rethinking communication, technology, and ourselves. Peter Lang, New York},
  pages={201--220},
  year={2018}
}

@book{darling2021new,
  title={The new breed: what our history with animals reveals about our future with robots},
  author={Darling, Kate},
  year={2021},
  publisher={Henry Holt and Company}
}

@inbook{friedman2021clothing,
author = {Friedman, Natalie and Love, Kari and LC, RAY and Sabin, Jenny E and Hoffman, Guy and Ju, Wendy},
title = {What Robots Need From Clothing},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462045},
abstract = { Most robots are unclothed. However, we believe that robot clothes present an underutilized opportunity for the field of designing interactive systems. Clothes can help robots become better robots––by helping them be useful in a new, wider array of contexts, or better adapt and function in the contexts they are already in. In this paper, we provide a foundation for a research area of robot clothing by speculating on its potential. We systematically present functional requirements of robot clothing, considerations, and parameters for robot clothing designers, as well as key reference cases of robots in clothes. We then discuss what robot clothes can do specifically for the field of designing interactive systems. },
booktitle = {Designing Interactive Systems Conference 2021},
pages = {1345–1355},
numpages = {11}
}

@inproceedings{macdorman2005mortality,
  title={Mortality salience and the uncanny valley},
  author={MacDorman, Karl F},
  booktitle={5th IEEE-RAS International Conference on Humanoid Robots, 2005.},
  pages={399--405},
  year={2005},
  organization={IEEE}
}

@article{breazeal2013crowdsourcing,
author = {Breazeal, Cynthia and DePalma, Nick and Orkin, Jeff and Chernova, Sonia and Jung, Malte},
title = {Crowdsourcing Human-Robot Interaction: New Methods and System Evaluation in a Public Environment},
year = {2013},
issue_date = {February 2013},
publisher = {Journal of Human-Robot Interaction Steering Committee},
volume = {2},
number = {1},
url = {https://doi.org/10.5898/JHRI.2.1.Breazeal},
doi = {10.5898/JHRI.2.1.Breazeal},
abstract = {Supporting a wide variety of interaction styles across a diverse set of people is
a significant challenge in human-robot interaction (HRI). In this work, we explore
a data-driven approach that relies on crowdsourcing as a rich source of interactions
that cover a wide repertoire of human behavior. We first develop an online game that
requires two players to collaborate to solve a task. One player takes the role of
a robot avatar and the other a human avatar, each with a different set of capabilities
that must be coordinated to overcome challenges and complete the task. Leveraging
the interaction data recorded in the online game, we present a novel technique for
data-driven behavior generation using case-based planning for a real robot. We compare
the resulting autonomous robot behavior against a Wizard of Oz base case condition
in a real-world reproduction of the online game that was conducted at the Boston Museum
of Science. Results of a post-study survey of participants indicate that the autonomous
robot behavior matched the performance of the human-operated robot in several important
measures. We examined video recordings of the real-world game to draw additional insights
as to how the novice participants attempted to interact with the robot in a loosely
structured collaborative task. We discovered that many of the collaborative interactions
were generated in the moment and were driven by interpersonal dynamics, not necessarily
by the task design. We explored using bids analysis as a meaningful construct to tap
into affective qualities of HRI. An important lesson from this work is that in loosely
structured collaborative tasks, robots need to be skillful in handling these in-the-moment
interpersonal dynamics, as these dynamics have an important impact on the affective
quality of the interaction for people. How such interactions dovetail with more task-oriented
policies is an important area for future work, as we anticipate such interactions
becoming commonplace in situations where personal robots perform loosely structured
tasks in interaction with people in human living spaces.},
journal = {J. Hum.-Robot Interact.},
month = feb,
pages = {82–111},
numpages = {30},
keywords = {bids analysis, humanoid dialog, crowdsourcing, human-robot teaming}
}

@article{hoorn2020medium,
  title={Theory of robot communication: I. The medium is the communication partner},
  author={Hoorn, Johan F},
  journal={International Journal of Humanoid Robotics},
  volume={17},
  number={06},
  pages={2050026},
  year={2020},
  publisher={World Scientific}
}







@article{henkel2014photo,
author = {Linda A. Henkel},
title ={Point-and-Shoot Memories: The Influence of Taking Photos on Memory for a Museum Tour},
journal = {Psychological Science},
volume = {25},
number = {2},
pages = {396-402},
year = {2014},
doi = {10.1177/0956797613504438},
    note ={PMID: 24311477},

URL = { 
        https://doi.org/10.1177/0956797613504438
    
},
eprint = { 
        https://doi.org/10.1177/0956797613504438
    
}
,
    abstract = { Two studies examined whether photographing objects impacts what is remembered about them. Participants were led on a guided tour of an art museum and were directed to observe some objects and to photograph others. Results showed a photo-taking-impairment effect: If participants took a photo of each object as a whole, they remembered fewer objects and remembered fewer details about the objects and the objects’ locations in the museum than if they instead only observed the objects and did not photograph them. However, when participants zoomed in to photograph a specific part of the object, their subsequent recognition and detail memory was not impaired, and, in fact, memory for features that were not zoomed in on was just as strong as memory for features that were zoomed in on. This finding highlights key differences between people’s memory and the camera’s “memory” and suggests that the additional attentional and cognitive processes engaged by this focused activity can eliminate the photo-taking-impairment effect. }
}


@article{soares2018photo,
title = {Forget in a Flash: A Further Investigation of the Photo-Taking-Impairment Effect},
journal = {Journal of Applied Research in Memory and Cognition},
volume = {7},
number = {1},
pages = {154-160},
year = {2018},
issn = {2211-3681},
doi = {https://doi.org/10.1016/j.jarmac.2017.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S2211368117301687},
author = {Julia S. Soares and Benjamin C. Storm},
keywords = {Photo-taking impairment, Offloading, Transactive memory, Snapchat},
abstract = {A photo-taking-impairment effect has been observed such that participants are less likely to remember objects they photograph than objects they only observe. According to the offloading hypothesis, taking photos allows people to offload organic memory onto the camera's prosthetic memory, which they can rely upon to “remember” for them. We tested this hypothesis by manipulating whether participants perceived photo-taking as capable of serving as a form of offloading. In Experiment 1, participants used the ephemeral photo application Snapchat. In Experiment 2, participants manually deleted photos after taking them. In both experiments, participants exhibited a significant photo-taking-impairment effect even though they did not expect to have access to the photos. In fact, the effect was just as large as when participants believed they would have access to the photos. These results suggest that offloading may not be the sole, or even primary, mechanism for the photo-taking-impairment effect.}
}

@article{fischer2021anthro,
author = {Fischer, Kerstin},
title = {Tracking Anthropomorphizing Behavior in Human-Robot Interaction},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
url = {https://doi.org/10.1145/3442677},
doi = {10.1145/3442677},
abstract = {Existing methodologies to describe anthropomorphism in human-robot interaction often rely either on specific one-time responses to robot behavior, such as keeping the robot's secret, or on post hoc measures, such as questionnaires. Currently, there is no method to describe the dynamics of people's behavior over the course of an interaction and in response to robot behavior. In this paper, I propose a method that allows the researcher to trace anthropomorphizing and non-anthropomorphizing responses to robots dynamically moment-by-moment over the course of human-robot interactions. I illustrate this methodology in a case study and find considerable variation between participants, but also considerable intrapersonal variation in the ways the robot is anthropomorphized. That is, people may respond to the robot as if it was another human in one moment and to its machine-like properties in the next. These findings may influence explanatory models of anthropomorphism.},
journal = {J. Hum.-Robot Interact.},
month = {oct},
articleno = {4},
numpages = {28},
keywords = {methodology, mindless transfer, computers-are-social-actors, Anthropomorphism, interaction analysis}
}


@article{ellestrom2018comm,
author = {Lars Elleström},
doi = {doi:10.1515/sem-2016-0024},
url = {https://doi.org/10.1515/sem-2016-0024},
title = {A medium-centered model of communication},
journal = {Semiotica},
number = {224},
volume = {2018},
year = {2018},
pages = {269--293}
}







@article{softrobo,
author = {Holland, Dónal P. and Park, Evelyn J. and Polygerinos, Panagiotis and Bennett, Gareth J. and Walsh, Conor J.},
title = {The Soft Robotics Toolkit: Shared Resources for Research and Design},
journal = {Soft Robotics},
volume = {1},
number = {3},
pages = {224-230},
year = {2014},
doi = {10.1089/soro.2014.0010},

URL = { 
        https://doi.org/10.1089/soro.2014.0010
    
},
eprint = { 
        https://doi.org/10.1089/soro.2014.0010
    
}
,
    abstract = { Abstract This article describes the development of the Soft Robotics Toolkit, a set of open access resources to support the design, fabrication, modeling, characterization, and control of soft robotic devices. The ultimate aim of the toolkit is to support researchers in building upon each other's work, and thereby advance the field of soft robotics. An additional aim is to support educators and encourage students to pursue careers in engineering and science by making the resources as accessible as possible. The toolkit was developed and refined through a series of pilot studies and user tests. Specifically, the resources were used by students in a project-based medical device design course; volunteers from a variety of backgrounds tested the toolkit and provided feedback, and soft robotics researchers used the collection of resources and contributed to its development. Throughout all user studies, qualitative data were collected and used to guide improvements to the toolkit. This process of testing and refinement has resulted in a website containing design documentation describing general hardware control platforms and specific soft robotic component designs. The online documentation includes downloadable computer-aided design (CAD) files, detailed multimedia protocols for the fabrication of soft devices, tutorials and scripts for modeling and analyzing soft actuators and sensors, and source code for controlling soft devices. Successive iterations of qualitative data gathering and redesign have confirmed that the toolkit documentation is sufficiently detailed to be useful for researchers from a wide range of backgrounds. To date, the focus of the toolkit has primarily been fluid-actuated robotic systems, but the plan is to expand it to support a wider range of soft robotic-enabling technologies. The toolkit is intended as a community resource, and all researchers working in this field are invited to guide its future development by providing feedback and contributing new content. }
}

@article{suguitan2022variable,
  title={What is it like to be a bot? Variable perspective embodied telepresence for crowdsourcing robot movements},
  author={Suguitan, Michael and Hoffman, Guy},
  journal={Personal and Ubiquitous Computing},
  pages={1--17},
  year={2022},
  publisher={Springer}
}

@inproceedings{jacob2012yeah,
  title={Yeah, I Talk to My Car, So What? Different Roles and Levels of Closeness in Person-Object Relationships},
  author={Jacob, RH and Tor{\'a}n, MM and Esteve, MC and others},
  booktitle={DS 73-2 Proceedings of the 2nd International conference on Design Creativity Volume 2},
  pages={29--36},
  year={2012}
}

@inproceedings{swaminathan2021comedy,
  author={Swaminathan, Janani and Akintoye, Jane and Fraune, Marlena R. and Knight, Heather},
  booktitle={2021 30th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)}, 
  title={Robots That Run their Own Human Experiments: Exploring Relational Humor with Multi-Robot Comedy}, 
  year={2021},
  volume={},
  number={},
  pages={1262--1268},
  doi={10.1109/RO-MAN50785.2021.9515324}
}

@article{pagliarini2009development,
  title={The development of robot art},
  author={Pagliarini, Luigi and Hautop Lund, Henrik},
  journal={Artificial Life and Robotics},
  volume={13},
  number={2},
  pages={401--405},
  year={2009},
  publisher={Springer}
}